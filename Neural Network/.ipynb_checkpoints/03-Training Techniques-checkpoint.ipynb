{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:30:28.376991Z",
     "start_time": "2021-03-16T02:30:25.757801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier初始化\n",
    "\n",
    "glorot认为每层的输出方差应等于输入方差，这样可以避免梯度消失或梯度爆炸，要实现这一点可以对每层的权重进行如下的初始化，主要有两种\n",
    "\n",
    "- **Glorot正态分布初始化方法**，也称作Xavier正态分布初始化，参数由0均值，标准差为sqrt(2 / (fan_in + fan_out))的正态分布产生，其中fan_in和fan_out是权重张量的扇入扇出（即输入和输出单元数目）\n",
    "- **Glorot均匀分布初始化方法**，又成Xavier均匀初始化，参数从[-limit, limit]的均匀分布产生，其中limit为sqrt(6 / (fan_in + fan_out))。fan_in为权值张量的输入单元数，fan_out是权重张量的输出单元数。keras默认的初始化方式是glorot均匀分布初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He初始化\n",
    "\n",
    "* He正态分布初始化方法，参数由0均值，标准差为sqrt(2 / fan_in) 的正态分布产生，其中fan_in权重张量的扇入\n",
    "* He均匀分布初始化方法，参数由[-limit, limit]的区间中均匀采样获得，其中limit=sqrt(6 / fan_in), fin_in是权重向量的输入单元数（扇入）\n",
    "* 与Xavier的区别是，He使用的fan_in, glorot使用的是fan_avg = (fan_in + fan_out)/2\n",
    "* 调用方法：在定义层的时候进行初始化设定，model.add(Dense(kernel_initializer='...', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeCun初始化\n",
    "\n",
    "* LeCun正态分布初始化方法，参数由0均值，标准差为stddev = sqrt(1 / fan_in)的正态分布产生\n",
    "* LeCun均匀分布初始化方法，参数由[-limit, limit]的区间中均匀采样获得，其中limit=sqrt(3 / fan_in), fin_in是权重向量的输入单元数（扇入）\n",
    "* 调用方法：在定义层的时候进行初始化设定，model.add(Dense(kernel_initializer='...', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非饱和激活函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "* 优点：ReLU解决了因饱和激活函是在饱和值处梯度消失问题。\n",
    "* 缺点：ReLU存在**Dying ReLU Problem**, 如果对ReLU的输入为负数时，神经元会死亡\n",
    "\n",
    "使用：\n",
    "`activation=\"relu\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "* 优点：Leaky ReLU可以解决Dying ReLU 问题，定义为$LeakyReLU_α(z) = max(αz, z)$ 其中超参数$α$定义函数泄露程度，它是z<0时的函数斜率，通常设置为0.01\n",
    "\n",
    "使用：\n",
    "```\n",
    "# 构建网络的时候，需要单独添加leaky relu层\n",
    "keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "keras.layers.LeakyReLU(),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Leaky ReLU\n",
    "\n",
    "* 在训练过程中在给定范围内随机选择α， 在测试过程中将其固定为平均值，可以减少过拟合的风险，当做正则化进行使用\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Leaky ReLU\n",
    "\n",
    "* PReLU在大型图像数据集上的性能明显优于ReLU,但是在较小数据集上存在过拟合风险\n",
    "\n",
    "使用：\n",
    "```\n",
    "# 构建网络的时候，需要单独添加leaky relu层\n",
    "keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "keras.layers.PReLU(),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU\n",
    "\n",
    "$$\n",
    "\\mathrm{ELU}_{\\alpha}(z)=\\left\\{\\begin{array}{ll}\n",
    "\\alpha(\\exp (z)-1) & \\text { if } z<0 \\\\\n",
    "z & \\text { if } z \\geq 0\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "* 当z<0时，取负值，使该单元的平均输出接近0，有助于缓解梯度消失问题\n",
    "* 当z<0时，具有非零梯度，从而避免了神经元死亡的问题\n",
    "* 超参数α，该值是当z为较大的负数时ELU函数毕竟的值，一般取1\n",
    "* 如果α=1，ELU函数在所有位置都是平滑的，有助于加速梯度下降，如下面图像所示\n",
    "\n",
    "![image-20210315171624532](images/image-20210315171624532.png)\n",
    "\n",
    "缺点：\n",
    "因为ELU中存在指数函数的缘故，因此计算会比ReLU慢，但是其在训练过程中有更多的收敛速度，可以弥补这种计算缓慢\n",
    "\n",
    "\n",
    "使用：\n",
    "`activation=\"elu\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled ELU\n",
    "\n",
    "* SELU的使用需要满足一些条件，目的使为了实现其自归一化：\n",
    "    * 输入特征必须使标准化的（均值为0， 标准差为1）\n",
    "    * 每个隐藏层的权重必须使用LeCun正态初始化，`kernel_initializer=\"lecun_normal\"`\n",
    "    * 网络架构必须是Sequential的，具有skip-connection的wide deep 网络无法保证自归一化\n",
    "    \n",
    "    \n",
    "使用：\n",
    "```\n",
    "keras.layers.Dense(300, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\"),\n",
    "```\n",
    "    \n",
    "    \n",
    "## 总结：\n",
    "\n",
    "* 网络能够保证自归一化：SELU优于ELU\n",
    "* 无法保证自归一化：ELU优于SELU\n",
    "* 关心运行时的延迟：使用Leaky ReLU\n",
    "* 网络过拟合：使用RReLU\n",
    "* 训练集很大：使用PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Leaky ReLU\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PReLU\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重用预训练层\n",
    "\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:30:30.801093Z",
     "start_time": "2021-03-16T02:30:30.372829Z"
    }
   },
   "outputs": [],
   "source": [
    "# 准备微调任务所需的数据\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:30:32.755508Z",
     "start_time": "2021-03-16T02:30:31.935982Z"
    }
   },
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"models/my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复制已有模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:30:34.351437Z",
     "start_time": "2021-03-16T02:30:34.305300Z"
    }
   },
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\", name='output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冻结预训练模型\n",
    "\n",
    "由于我要做二分类任务，此时，只需要对最后一层进行训练即可，但是新的输出层是随机初始化的，可能在前几个轮次内产生较大的错误，产生的梯度可能会破坏重用的权重。一种方法是在前几个轮次冻结重用的层，给新层一些时间来学习合理的权重。因此冻结除预训练层最后一层以外的其他层，使其不可训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:30:36.580679Z",
     "start_time": "2021-03-16T02:30:36.570728Z"
    }
   },
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 少轮次训练最后一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:30:41.736440Z",
     "start_time": "2021-03-16T02:30:39.642246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 2s 153ms/step - loss: 1.6458 - accuracy: 0.0904 - val_loss: 1.4812 - val_accuracy: 0.1420\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 1.5128 - accuracy: 0.1219 - val_loss: 1.3532 - val_accuracy: 0.1826\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 1.3771 - accuracy: 0.1834 - val_loss: 1.2357 - val_accuracy: 0.2272\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 1.2772 - accuracy: 0.1801 - val_loss: 1.1307 - val_accuracy: 0.2688\n"
     ]
    }
   ],
   "source": [
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解冻其他层参数并再次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:30:47.982529Z",
     "start_time": "2021-03-16T02:30:45.922718Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 32ms/step - loss: 0.9689 - accuracy: 0.3816 - val_loss: 0.7406 - val_accuracy: 0.5456\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.6703 - accuracy: 0.6132 - val_loss: 0.5308 - val_accuracy: 0.7110\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4626 - accuracy: 0.8039 - val_loss: 0.4049 - val_accuracy: 0.8469\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.3299 - accuracy: 0.9042 - val_loss: 0.3240 - val_accuracy: 0.9087\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2663 - accuracy: 0.9384 - val_loss: 0.2718 - val_accuracy: 0.9300\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.2200 - accuracy: 0.9510 - val_loss: 0.2337 - val_accuracy: 0.9483\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1983 - accuracy: 0.9627 - val_loss: 0.2067 - val_accuracy: 0.9594\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1734 - accuracy: 0.9830 - val_loss: 0.1842 - val_accuracy: 0.9716\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1375 - accuracy: 0.9875 - val_loss: 0.1665 - val_accuracy: 0.9757\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1344 - accuracy: 0.9938 - val_loss: 0.1529 - val_accuracy: 0.9777\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1158 - accuracy: 0.9981 - val_loss: 0.1414 - val_accuracy: 0.9777\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1097 - accuracy: 0.9950 - val_loss: 0.1321 - val_accuracy: 0.9787\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0959 - accuracy: 0.9981 - val_loss: 0.1239 - val_accuracy: 0.9787\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0971 - accuracy: 0.9950 - val_loss: 0.1173 - val_accuracy: 0.9828\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0791 - accuracy: 0.9981 - val_loss: 0.1111 - val_accuracy: 0.9858\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0741 - accuracy: 0.9981 - val_loss: 0.1056 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量归一化\n",
    "\n",
    "合适的初始化方法和正确的激活函数选择并不能完全消除vanish问题，批量归一化可以很好的解决该问题\n",
    "\n",
    "数学表达：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\boldsymbol{\\mu}_{B}=\\frac{1}{m_{B}} \\sum_{i=1}^{m_{B}} \\mathbf{x}^{(i)} \\\\\n",
    "{\\boldsymbol{\\sigma}}_{B}^{2}=\\frac{1}{m_{B}} \\sum_{i=1}^{m_{B}}\\left(\\mathbf{x}^{(i)}-\\boldsymbol{\\mu}_{B}\\right)^{2} \\\\\n",
    "\\widehat{\\mathbf{x}}^{(i)}=\\frac{\\mathbf{x}^{(i)}-\\boldsymbol{\\mu}_{B}}{\\sqrt{\\boldsymbol{\\sigma}_{B}^{2}+\\epsilon}} \\\\\n",
    "\\mathbf{z}^{(i)}=\\boldsymbol{\\gamma} \\otimes \\widehat{\\mathbf{x}}^{(i)}+\\boldsymbol{\\beta}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "* $m_B$是小批量中的实例数量\n",
    "* $x_(i)$是实例i的零中心和归一化输入的向量\n",
    "* $\\gamma$是该层的输出缩放向量，用于控制均值\n",
    "* $\\beta$是该层的输出偏移向量，用于控制方差\n",
    "\n",
    "需要学习的参数有：$\\gamma$，$\\beta$\n",
    "\n",
    "使用方法：\n",
    "\n",
    "* 训练期间：\n",
    "    * 每个隐藏层的激活函数之前或之后添加一个批量归一化操作，该操作可以使模型学习到各层输入的最佳缩放和均值\n",
    "    * 可以添加到输入层，此时便可以无需归一化训练集（例如StandardScaler）\n",
    "\n",
    "* 测试期间：\n",
    "\n",
    "    * 因为测试数据集有时很小的缘故，可以等到训练结束后，通过神经网络运行整个训练集，计算BN层每个输入的均值和方差，在进行预测时，可以使用这些均值和方差，而不是一个批次的输入均值和方差\n",
    "\n",
    "\n",
    "优点：\n",
    "\n",
    "* 使用BN后，可以使用饱和激活函数例如tanh\n",
    "* 可以使用更大的学习率\n",
    "\n",
    "缺点：\n",
    "* 增加了模型的复杂性，额外的计算，预测速度较慢，虽然计算慢了，但是收敛更多，总得显示速度更快\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:23:38.938743Z",
     "start_time": "2021-03-16T01:23:37.713732Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:23:57.685103Z",
     "start_time": "2021-03-16T01:23:57.671140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何在激活函数之前使用BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:27:41.748444Z",
     "start_time": "2021-03-16T01:27:41.675634Z"
    }
   },
   "outputs": [],
   "source": [
    "# 由于BN层每个输入都包含一个偏移参数，因此可以从上一层中删除偏置项即use_bias = False\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度裁剪\n",
    "\n",
    "核心思想：反向传播期间，裁剪梯度，使他们永远不会超过某个阀值\n",
    "\n",
    "应用场景：常用于RNN，对于其他类型网络，BN就足够了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:46:09.752006Z",
     "start_time": "2021-03-16T01:46:09.736048Z"
    }
   },
   "source": [
    "```python\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "```\n",
    "\n",
    "该优化器会将梯度向量的每个分量都裁剪为-1和1之间，注意，可能会改变梯度向量的方向，如果要确保梯度裁剪不更改梯度向量的方向，应该通过设置clipnorm，而不是clipvalue\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "核心思想：关心先前的梯度是什么，通过动量向量$m$来跟踪上一个时刻的梯度，通过动量$\\beta$来控制摩擦，为0时表示高摩擦，相当于原始的梯度，为1时表示无摩擦，通常0.9效果最好\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\mathbf{m} \\leftarrow \\beta \\mathbf{m}-\\eta \\nabla_{\\theta} J(\\boldsymbol{\\theta}) \\\\\n",
    "\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}+\\mathbf{m}\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:31:30.186708Z",
     "start_time": "2021-03-16T02:31:30.171789Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov\n",
    "\n",
    "核心思想：稍微提前测量梯度，通过在$\\theta+\\beta m$而不是在$\\theta$\n",
    "\n",
    "比常规动量优化要快。\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\mathbf{m} \\leftarrow \\beta \\mathbf{m}-\\eta \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}+\\boldsymbol{\\beta} \\mathbf{m}) \\\\\n",
    "\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}+\\mathbf{m}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "核心思想：提前纠正梯度方向，使其更多的指向全局最优解，实现方式通过考虑各个分量的平方并按比例缩小梯度向量并累加\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\mathbf{s} \\leftarrow \\mathbf{s}+\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\otimes \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\\\\n",
    "\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}-\\eta \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\oslash \\sqrt{\\mathbf{s}+\\epsilon}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "该算法会自动降低学习率，因此是自适应的，几乎不需要调整学习率，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:50:28.043317Z",
     "start_time": "2021-03-16T02:50:28.028385Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "因为AdaGrad有时下降的太快，永远不会收敛到全局最优解的风险，因为AdaGrad累加了所有时刻的梯度，因此RMSProp的思想是只累加最近迭代中的梯度, 最近的度量通过下面公式的衰减率$\\beta$衡量。\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\mathbf{s} \\leftarrow \\beta \\mathbf{s}+(1-\\beta) \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\otimes \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\\\\n",
    "\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}-\\eta \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\oslash \\sqrt{\\mathbf{s}+\\epsilon}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "衰减率$\\beta$通常设为0.9，即下面代码中rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:54:50.122496Z",
     "start_time": "2021-03-16T02:54:50.118417Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "核心思想：同时考虑动量优化即RMSProp,即考虑过去的梯度，又考虑过去梯度的方向\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\mathbf{m} \\leftarrow \\beta_{1} \\mathbf{m}-\\left(1-\\beta_{1}\\right) \\nabla_{\\theta} J(\\boldsymbol{\\theta}) \\\\\n",
    "\\mathbf{s} \\leftarrow \\beta_{2} \\mathbf{s}+\\left(1-\\beta_{2}\\right) \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\otimes \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\\\\n",
    "\\widehat{\\mathbf{m}} \\leftarrow \\frac{\\mathbf{m}}{1-\\beta_{1}^{t}} \\\\\n",
    "\\widehat{\\mathbf{s}} \\leftarrow \\frac{\\mathbf{s}}{1-\\beta_{2}^{t}} \\\\\n",
    "\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}+\\eta \\widehat{\\mathbf{m}} \\oslash \\sqrt{\\hat{\\mathbf{s}}+\\epsilon}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- 动量衰减超参数$\\beta_1$通常初始化为0.9\n",
    "- 缩放衰减超参数通常初始化为0.999\n",
    "\n",
    "Adam,是自适应学习率算法，学习率可以使用默认值0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:59:10.845070Z",
     "start_time": "2021-03-16T02:59:10.832104Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nadam\n",
    "\n",
    "核心思想：Adam优化 + Nesterov技巧，即提前考虑梯度\n",
    "\n",
    "Nadam总体上胜过Adam,但是有时不如RMSProp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:00:55.325566Z",
     "start_time": "2021-03-16T03:00:55.314595Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率调度\n",
    "\n",
    "- 使用恒定学习率的策略：通常将恒定学习率从很小的值呈指数级增大到很大的值，再查看学习曲线并选择一个学习率略低于学习曲线开始回升的时刻的学习率，然后重新初始化模型开始训练\n",
    "\n",
    "\n",
    "- 使用动态学习率的策略：从一个较大的学习率开始，一旦训练没有取得进展就降低它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 幂调度\n",
    "\n",
    "特点：越来越缓慢的降低学习率\n",
    "\n",
    "```lr = lr0 / (1 + steps / s)**c```\n",
    "* Keras uses `c=1` and `s = 1 / decay`\n",
    "\n",
    "![image-20210316110724494](images/image-20210316110724494.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:06:32.526626Z",
     "start_time": "2021-03-16T03:06:32.512658Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指数调度\n",
    "\n",
    "特点：学习率每s步降低10倍\n",
    "\n",
    "```lr = lr0 * 0.1**(epoch / s)```\n",
    "\n",
    "![image-20210316110908159](images/image-20210316110908159.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:13:27.369632Z",
     "start_time": "2021-03-16T03:13:27.354671Z"
    }
   },
   "outputs": [],
   "source": [
    "# step1:创建一个采用当前轮次并返回学习率的函数，该函数会根据不同的学习率和s,返回不同的学习率调度函数\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:14:50.682438Z",
     "start_time": "2021-03-16T03:14:50.669473Z"
    }
   },
   "outputs": [],
   "source": [
    "# step2: 创建一个LearningRateScheduler回调函数，为其提供学习率调度函数\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分段恒定调度\n",
    "\n",
    "![image-20210316112237052](images/image-20210316112237052.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:22:08.555150Z",
     "start_time": "2021-03-16T03:22:08.544206Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建一个可以根据不同epoch返回不同学习率的函数，然后将函数封装到一个函数中，该函数可以不同过硬编码进行分段的具体值\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:22:22.722202Z",
     "start_time": "2021-03-16T03:22:22.707241Z"
    }
   },
   "outputs": [],
   "source": [
    "# step2: 创建一个LearningRateScheduler回调函数，为其提供学习率调度函数\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能调度\n",
    "\n",
    "每N步测量一次验证误差，并且当误差停止下降时，将学习率降低$\\lambda$倍\n",
    "\n",
    "![image-20210316112553249](images/image-20210316112553249.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:26:01.710825Z",
     "start_time": "2021-03-16T03:26:01.695866Z"
    }
   },
   "outputs": [],
   "source": [
    "# 每5步测量一次验证误差，并且当误差停止下降时，将学习率乘以0.5倍\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1周期调度\n",
    "\n",
    "与上面的学习率调度（即从大到小）不同的是，1周期调度从提高学习率$\\eta_0$开始，在训练中途线性增长到$\\eta_1$, 然后在训练的后半部分将学习率再次线性降低到$\\eta_0$\n",
    "\n",
    "使用方法：先使用找最优恒定学习率的方法，找到$\\eta_1$，然后将其除以10，即可得到$\\eta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start 初步训练寻找最大学习率\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = len(X) // batch_size * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "\n",
    "# end ----- 初步结束\n",
    "\n",
    "\n",
    "# 构建1周期学习率调度函数\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)\n",
    "        \n",
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(len(X_train) // batch_size * n_epochs, max_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l1(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 And L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l1_l2(0.1, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最佳实践\n",
    "\n",
    "如果想将相同的正则化函数应用于网络中所有层，又不想重复性的写上面的部分，可以使用functools.partial()函数封装，具体如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "在实践中，通常只对第一层至第三层(不包括输出层)中的神经元应用dropout\n",
    "\n",
    "重要：训练后，需要将每个输入连接权重乘以保留概率（1-p）\n",
    "\n",
    "注意：基于SELU激活函数应使用alpha dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "一般的DNN网络，考虑如下设置：\n",
    "\n",
    "| 超参数类型 | 默认值                                 |\n",
    "| ---------- | -------------------------------------- |\n",
    "| 初始化     | He初始化                               |\n",
    "| 激活函数   | ELU                                    |\n",
    "| 归一化     | 浅层网络：不需要；深度网络：批量归一化 |\n",
    "| 正则化     | 提前停止（如果需要，可加l2)            |\n",
    "| 优化器     | 动量优化(or RMSProp or Nadam)          |\n",
    "| 学习率调度 | 1周期                                  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "自归一化网络的DNN，考虑如下设置：\n",
    "\n",
    "\n",
    "| 超参数类型 | 默认值                        |\n",
    "| ---------- | ----------------------------- |\n",
    "| 初始化     | Lecun初始化                   |\n",
    "| 激活函数   | SELU                          |\n",
    "| 归一化     | 不需要                        |\n",
    "| 正则化     | 如果需要：alpha dropout       |\n",
    "| 优化器     | 动量优化(or RMSProp or Nadam) |\n",
    "| 学习率调度 | 1周期                         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
