{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T04:44:55.080043Z",
     "start_time": "2021-03-15T04:44:52.532488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 介绍\n",
    "\n",
    "\n",
    "> keras与tf.keras的区别：keras多后端，tf.keras单后端（只能是TensorFlow）下面的教程中使用的是tf.keras, 因此它可以使用处了keras以外的其他功能\n",
    "\n",
    "![image-20210310093333481](images/image-20210310093333481.png)\n",
    "\n",
    "导入方法：\n",
    "```python\n",
    "from tensorflow import keras\n",
    "```\n",
    "\n",
    "keras 在使用过程中，主要考虑以下几点：\n",
    "\n",
    "1. 模型创建方法\n",
    "2. 模型细节查看\n",
    "3. 损失函数，优化器定义，模型编译\n",
    "4. 训练，评估及预测\n",
    "5. 保存和还原模型\n",
    "6. 使用回调函数\n",
    "7. Tensorbload可视化\n",
    "8. 参数调整\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型创建方法\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 数据准备\n",
    "\n",
    "1. fetch_california_housing：加载加州住房数据集， 该数据集仅包含数字特征，没有缺失值\n",
    "2. train_test_split: 分割数据集为训练集，验证集，测试集\n",
    "3. StandardScaler：标准化数据集，x-μ/σ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:20:13.908557Z",
     "start_time": "2021-03-14T20:20:13.616127Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential API\n",
    "\n",
    "最简单的keras模型，仅由顺序连接的单层堆栈组成，又称为顺序API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:23:30.907762Z",
     "start_time": "2021-03-14T20:23:29.698887Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:23:58.425820Z",
     "start_time": "2021-03-14T20:23:44.643230Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 2.2656 - val_loss: 0.8556\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7413 - val_loss: 0.6531\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6604 - val_loss: 0.6099\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6245 - val_loss: 0.5658\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5770 - val_loss: 0.5355\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5609 - val_loss: 0.5173\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5500 - val_loss: 0.5081\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5200 - val_loss: 0.4799\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5051 - val_loss: 0.4690\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4910 - val_loss: 0.4656\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4794 - val_loss: 0.4482\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4656 - val_loss: 0.4479\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4693 - val_loss: 0.4296\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4537 - val_loss: 0.4233\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4586 - val_loss: 0.4175\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4612 - val_loss: 0.4123\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4449 - val_loss: 0.4071\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4407 - val_loss: 0.4037\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4184 - val_loss: 0.4000\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4128 - val_loss: 0.3969\n",
      "162/162 [==============================] - 0s 794us/step - loss: 0.4212\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单输入单输出\n",
    "\n",
    "![image-20210314172807709](images/image-20210314172807709.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:25:36.475789Z",
     "start_time": "2021-03-14T20:25:36.425931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           270         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           930         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            39          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:25:55.485618Z",
     "start_time": "2021-03-14T20:25:41.237624Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 3.2565 - val_loss: 0.6913\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6795 - val_loss: 0.9451\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6284 - val_loss: 0.6633\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5929 - val_loss: 0.5284\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5004\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5313 - val_loss: 0.5900\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5207 - val_loss: 0.5885\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4938 - val_loss: 0.4689\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4789 - val_loss: 0.5304\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4728 - val_loss: 0.5451\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4614 - val_loss: 0.4989\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4521 - val_loss: 0.7246\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4521 - val_loss: 0.4204\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4420 - val_loss: 0.4463\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4467 - val_loss: 0.4170\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4471 - val_loss: 0.4494\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4312 - val_loss: 0.4020\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4314 - val_loss: 0.3991\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4096 - val_loss: 0.4352\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4032 - val_loss: 0.3965\n",
      "162/162 [==============================] - 0s 914us/step - loss: 0.4165\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多输入单输出\n",
    "\n",
    "假设通过宽路径送入5个特征，深路径送入6个特征\n",
    "\n",
    "![image-20210314172915275](images/image-20210314172915275.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:35:22.211973Z",
     "start_time": "2021-03-14T20:35:22.205960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:35:56.768837Z",
     "start_time": "2021-03-14T20:35:56.759877Z"
    }
   },
   "outputs": [],
   "source": [
    "# 为input_A, input_B拆分数据，此时因为是多输入单输出，所以输入数据需要调整，如下\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, : 5], X_valid[:, 2: ]\n",
    "X_test_A, X_test_B = X_test[:, : 5], X_test[:, 2: ]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:35:57.641728Z",
     "start_time": "2021-03-14T20:35:57.599481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "deep_input (InputLayer)         [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           210         deep_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "wide_input (InputLayer)         [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           930         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 35)           0           wide_input[0][0]                 \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            36          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,176\n",
      "Trainable params: 1,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:36:13.622473Z",
     "start_time": "2021-03-14T20:35:58.418573Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.8482 - val_loss: 1.6660\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8838 - val_loss: 0.7827\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7046 - val_loss: 0.6340\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6419 - val_loss: 0.5860\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5903 - val_loss: 0.5473\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5560 - val_loss: 0.5176\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5509 - val_loss: 0.4939\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5221 - val_loss: 0.4727\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4935 - val_loss: 0.4567\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4789 - val_loss: 0.4444\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4678 - val_loss: 0.4382\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4579 - val_loss: 0.4286\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4542 - val_loss: 0.4266\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4512 - val_loss: 0.4229\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4545 - val_loss: 0.4193\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4533 - val_loss: 0.4136\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4432 - val_loss: 0.4147\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4438 - val_loss: 0.4203\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4224 - val_loss: 0.4141\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4141 - val_loss: 0.4218\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4261\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多输入多输出\n",
    "\n",
    "需要多个输出的场景：\n",
    "1. 基于同一数据的多个独立任务，当然可以为每个任务训练一个神经网络，但是在许多情况下，通过训练每个任务一个输出的单个神经网络效果更好\n",
    "2. 训练约束：例如在神经网络结构中添加一些辅助输出，以确保网络的主要部分自己能学习到有用的东西，而不依赖网络的其余部分\n",
    "3. 在图片中定位和分类，其中定位是一个回归问题例如长宽，同时分类是一个分类任务\n",
    "\n",
    "![image-20210314173755752](images/image-20210314173755752.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:42:06.080387Z",
     "start_time": "2021-03-14T20:42:06.069575Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:42:28.984542Z",
     "start_time": "2021-03-14T20:42:28.939468Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:45:08.450307Z",
     "start_time": "2021-03-14T20:44:44.823902Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 4.0679 - main_output_loss: 3.9348 - aux_output_loss: 5.2663 - val_loss: 3.1300 - val_main_output_loss: 2.9687 - val_aux_output_loss: 4.5822\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.2235 - main_output_loss: 0.9928 - aux_output_loss: 3.3000 - val_loss: 1.4616 - val_main_output_loss: 1.1331 - val_aux_output_loss: 4.4187\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.9123 - main_output_loss: 0.7692 - aux_output_loss: 2.2002 - val_loss: 1.0711 - val_main_output_loss: 0.7463 - val_aux_output_loss: 3.9935\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7935 - main_output_loss: 0.6889 - aux_output_loss: 1.7349 - val_loss: 0.9313 - val_main_output_loss: 0.6336 - val_aux_output_loss: 3.6108\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7327 - main_output_loss: 0.6391 - aux_output_loss: 1.5745 - val_loss: 0.8455 - val_main_output_loss: 0.5774 - val_aux_output_loss: 3.2583\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6773 - main_output_loss: 0.5923 - aux_output_loss: 1.4424 - val_loss: 0.7928 - val_main_output_loss: 0.5531 - val_aux_output_loss: 2.9504\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6692 - main_output_loss: 0.5856 - aux_output_loss: 1.4215 - val_loss: 0.7509 - val_main_output_loss: 0.5389 - val_aux_output_loss: 2.6586\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6222 - main_output_loss: 0.5446 - aux_output_loss: 1.3208 - val_loss: 0.7098 - val_main_output_loss: 0.5229 - val_aux_output_loss: 2.3923\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5971 - main_output_loss: 0.5229 - aux_output_loss: 1.2648 - val_loss: 0.6779 - val_main_output_loss: 0.5122 - val_aux_output_loss: 2.1690\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5806 - main_output_loss: 0.5071 - aux_output_loss: 1.2419 - val_loss: 0.6584 - val_main_output_loss: 0.5155 - val_aux_output_loss: 1.9446\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5538 - main_output_loss: 0.4835 - aux_output_loss: 1.1867 - val_loss: 0.6171 - val_main_output_loss: 0.4873 - val_aux_output_loss: 1.7850\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5397 - main_output_loss: 0.4731 - aux_output_loss: 1.1382 - val_loss: 0.5965 - val_main_output_loss: 0.4811 - val_aux_output_loss: 1.6350\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5275 - main_output_loss: 0.4669 - aux_output_loss: 1.0729 - val_loss: 0.5688 - val_main_output_loss: 0.4631 - val_aux_output_loss: 1.5202\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5179 - main_output_loss: 0.4587 - aux_output_loss: 1.0513 - val_loss: 0.5438 - val_main_output_loss: 0.4460 - val_aux_output_loss: 1.4241\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5127 - main_output_loss: 0.4564 - aux_output_loss: 1.0188 - val_loss: 0.5234 - val_main_output_loss: 0.4332 - val_aux_output_loss: 1.3350\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5144 - main_output_loss: 0.4585 - aux_output_loss: 1.0172 - val_loss: 0.5061 - val_main_output_loss: 0.4226 - val_aux_output_loss: 1.2581\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4965 - main_output_loss: 0.4422 - aux_output_loss: 0.9860 - val_loss: 0.4912 - val_main_output_loss: 0.4130 - val_aux_output_loss: 1.1954\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4964 - main_output_loss: 0.4431 - aux_output_loss: 0.9756 - val_loss: 0.4789 - val_main_output_loss: 0.4054 - val_aux_output_loss: 1.1408\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4693 - main_output_loss: 0.4208 - aux_output_loss: 0.9056 - val_loss: 0.4707 - val_main_output_loss: 0.4020 - val_aux_output_loss: 1.0885\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4645 - main_output_loss: 0.4180 - aux_output_loss: 0.8828 - val_loss: 0.4633 - val_main_output_loss: 0.3987 - val_aux_output_loss: 1.0445\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4664 - main_output_loss: 0.4219 - aux_output_loss: 0.8666\n"
     ]
    }
   ],
   "source": [
    "# 每个输出都需要自己的损失，如果传递单个损失，会将所有输出使用同一个损失，这样不好，\n",
    "# 同时如果我主要关注main-output的损失，可以为不同的损失添加权重，即loss_weights\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1],\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train],\n",
    "                    epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "\n",
    "# 评估模型时，keras将返回总损失以及所有单个损失\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubClass API\n",
    "\n",
    "一些模型涉及循环，变化的形状，条件分支，和其他动态行为，子类API会很实用\n",
    "\n",
    "```python\n",
    "\n",
    "class SomeModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        # 构造器\n",
    "        # ...\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # some code\n",
    "        \n",
    "```\n",
    "\n",
    "* 构造器中创建所需的层，相当于各个层节点\n",
    "* call函数中执行每个层上的计算，相当于层与层之间的连接关系\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:51:50.095774Z",
     "start_time": "2021-03-14T20:51:50.086797Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[: 3], X_test_B[: 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:52:01.132216Z",
     "start_time": "2021-03-14T20:52:01.117233Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T20:52:17.783776Z",
     "start_time": "2021-03-14T20:52:05.232829Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 3.5897 - output_1_loss: 3.5051 - output_2_loss: 4.3518 - val_loss: 3.2359 - val_output_1_loss: 2.9913 - val_output_2_loss: 5.4378\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.1428 - output_1_loss: 0.9873 - output_2_loss: 2.5428 - val_loss: 1.6802 - val_output_1_loss: 1.3841 - val_output_2_loss: 4.3448\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.8694 - output_1_loss: 0.7598 - output_2_loss: 1.8562 - val_loss: 1.0847 - val_output_1_loss: 0.8147 - val_output_2_loss: 3.5147\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7677 - output_1_loss: 0.6827 - output_2_loss: 1.5330 - val_loss: 0.8951 - val_output_1_loss: 0.6539 - val_output_2_loss: 3.0659\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7094 - output_1_loss: 0.6324 - output_2_loss: 1.4028 - val_loss: 0.8030 - val_output_1_loss: 0.5821 - val_output_2_loss: 2.7910\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6569 - output_1_loss: 0.5897 - output_2_loss: 1.2615 - val_loss: 0.7429 - val_output_1_loss: 0.5482 - val_output_2_loss: 2.4957\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6547 - output_1_loss: 0.5908 - output_2_loss: 1.2299 - val_loss: 0.6969 - val_output_1_loss: 0.5233 - val_output_2_loss: 2.2595\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6116 - output_1_loss: 0.5530 - output_2_loss: 1.1387 - val_loss: 0.6668 - val_output_1_loss: 0.5017 - val_output_2_loss: 2.1534\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5896 - output_1_loss: 0.5333 - output_2_loss: 1.0966 - val_loss: 0.6428 - val_output_1_loss: 0.4854 - val_output_2_loss: 2.0598\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5739 - output_1_loss: 0.5173 - output_2_loss: 1.0828 - val_loss: 0.6175 - val_output_1_loss: 0.4700 - val_output_2_loss: 1.9443\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.5532 - output_1_loss: 0.5006 - output_2_loss: 1.0260\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000205054D0E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1],\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train),\n",
    "                    epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    (X_test_A, X_test_B), (y_test, y_test))\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型细节查看\n",
    "\n",
    "## 网络层\n",
    "\n",
    "- `model.layers`会返回层列表\n",
    "- `model.get_layer(...)`通过名称获取层\n",
    "\n",
    "\n",
    "## 网络权重\n",
    "\n",
    "- `layer_name.get_weights`访问给定层的权重（包含weights和bias）\n",
    "- `layer_name.set_weights`设置给定层的权重（包含weights和bias）\n",
    "\n",
    "注意：Dense层已经默认设置了随机化权重，如果需要使用其他初始化方法，创建层时使用`kernal_initializer`\n",
    "\n",
    "\n",
    "## 查看参数\n",
    "\n",
    "- `model.summay()`会显示模型所有层，包括每个层的名称，输出形状，参数数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数，优化器定义，模型编译\n",
    "\n",
    "## 损失函数定义\n",
    "\n",
    "- 损失函数的定义通过`model.compile(loss=...)`来进行定义\n",
    "- 有两种方式给定：\n",
    "    - 一种是给定缩写名称，例如mean squared error可以是`loss='mean_squared_error'`\n",
    "    - 另一种是给定keras的引用路径，`loss=keras.losses.MeanSquaredError`\n",
    "\n",
    "损失函数的完整列表参照文档：[keras.losses](https://keras.io/api/losses/)\n",
    "\n",
    "\n",
    "## 优化器定义\n",
    "- 优化器的定义通过`model.compile(optimizer=...)`来进行定义\n",
    "- 有两种方式给定：\n",
    "    - 一种是给定缩写名称，例如sgd可以是`optimizer='sgd'`\n",
    "    - 另一种是给定keras的引用路径，`optimizer=keras.optimizers.SGD()`,如果是这种方式可以通过加入参数`lr=...`来调整学习率\n",
    "- \n",
    "\n",
    "优化器的完整列表参照文档：[keras.optimizers](https://keras.io/api/optimizers/)\n",
    "\n",
    "\n",
    "## 评价指标定义\n",
    "- 评价指标定义通过`model.compile(metrics=[...])`来进行定义\n",
    "- 有两种方式给定：\n",
    "    - 一种是给定缩写名称，例如sgd可以是`metrics=['accuracy']`\n",
    "    - 另一种是给定keras的引用路径，`metrics=[keras.metrics.Accuracy]`\n",
    "\n",
    "评价指标的完整列表参照文档：[keras.metrics](https://keras.io/api/metrics/)\n",
    "\n",
    "\n",
    "\n",
    "## 模型编译\n",
    "\n",
    "- `model.compile(loss=..., optimizer=..., metrics=[...], etc)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练，评估及预测\n",
    "\n",
    "## 训练\n",
    "\n",
    "- `model.fit(training data, training label, epoch=..., validation_data=(valid data, valid label))`\n",
    "- 如果使用`validation_split=0.x`,则不需要使用`validation_data`\n",
    "- 如果label不平衡，可以增加`class_weight`参数为不同的类别增加权重\n",
    "- 调用`model.fit`后，会返回一个`history`对象，里面的`history.history`可以用于绘制epoch-loss学习曲线, 例如`pd.DataFrame(history.history).plot(figsize=(8,5))`\n",
    "\n",
    "\n",
    "## 评估\n",
    "\n",
    "- `model.evaluate(test data, test label)`\n",
    "\n",
    "\n",
    "## 预测\n",
    "\n",
    "- `model.predict(new data)` or `model.predict_classes(new data)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存和还原模型\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T22:33:34.130330Z",
     "start_time": "2021-03-14T22:33:26.797638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 3.3697 - val_loss: 0.7125\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6964 - val_loss: 0.6880\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6167 - val_loss: 0.5802\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5845 - val_loss: 0.5166\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5321 - val_loss: 0.4895\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5083 - val_loss: 0.4947\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5044 - val_loss: 0.4860\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4813 - val_loss: 0.4550\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4628 - val_loss: 0.4412\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4549 - val_loss: 0.4377\n",
      "162/162 [==============================] - 0s 862us/step - loss: 0.4382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_new = X_test[:3]\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras使用HDF5保存模型结构（仅限于顺序API和函数式API）\n",
    "    - 包括每一层的超参数\n",
    "    - 每一层的连接权重和偏置\n",
    "    - 优化器等（以及各自的超参数或可能的状态）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T22:34:25.861816Z",
     "start_time": "2021-03-14T22:34:25.830901Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"models/my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 子类API至少可以使用`save_weights()`保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T22:34:32.540486Z",
     "start_time": "2021-03-14T22:34:32.507550Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"models/my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 还原模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T22:35:11.620815Z",
     "start_time": "2021-03-14T22:35:11.525881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020503F333A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5398443],\n",
       "       [1.6505727],\n",
       "       [3.0097804]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/my_keras_model.h5\")\n",
    "\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T22:36:54.174492Z",
     "start_time": "2021-03-14T22:36:54.150556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2050eb7d070>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"models/my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回调函数\n",
    "\n",
    "- fit()方法使用callbacks的方式，`fit(..., callbacks=[...])`\n",
    "- 目的是为了在训练过程中执行某些操作，例如定期模型保存，提前停止，tensorboard等\n",
    "- 定义方式是`keras.callbacks.xxx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T22:40:10.472032Z",
     "start_time": "2021-03-14T22:40:09.676385Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_new = X_test[:3]\n",
    "\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCheckpoint - 定期保存模型\n",
    "\n",
    "- 训练期间，`ModelCheckpoint`回调会定期保存检查点\n",
    "- 设置`save_best_only=True`时，只有在验证集上模型性能达到目前最好时，才会保存模型，这样就不用担心训练过长而过拟合训练集，只需还原最后一个模型，即为验证集中最佳模型\n",
    "- 相当于实现了提前停止\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T22:41:45.769999Z",
     "start_time": "2021-03-14T22:41:37.130116Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 3.3697 - val_loss: 0.7125\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6964 - val_loss: 0.6880\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6167 - val_loss: 0.5802\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5845 - val_loss: 0.5166\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5321 - val_loss: 0.4895\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5083 - val_loss: 0.4947\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5044 - val_loss: 0.4860\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4813 - val_loss: 0.4550\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4628 - val_loss: 0.4412\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4549 - val_loss: 0.4377\n",
      "162/162 [==============================] - 0s 961us/step - loss: 0.4382\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"models/my_keras_model.h5\", \n",
    "                                                save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "\n",
    "# rollback to best model\n",
    "model = keras.models.load_model(\"models/my_keras_model.h5\")\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EarlyStopping - 提前停止\n",
    "\n",
    "- 另一种实现提前停止的方法是使用`EarlyStopping`回调\n",
    "- 该回调的实现提前停止的方式是如果在多个轮次（通过`patience`定义）的验证集上没有任何进展，它将中断训练，并且回滚到最佳模型\n",
    "- 使用方式：通常会和`ModelCheckpoint`回调一起使用，既可以保存最优模型，又可以防止浪费计算资源\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T22:54:12.617099Z",
     "start_time": "2021-03-14T22:53:06.756394Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4578 - val_loss: 0.4109\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4430 - val_loss: 0.4267\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4377 - val_loss: 0.3996\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4361 - val_loss: 0.3939\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4204 - val_loss: 0.3889\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4112 - val_loss: 0.3866\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4226 - val_loss: 0.3860\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4135 - val_loss: 0.3793\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4039 - val_loss: 0.3746\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4023 - val_loss: 0.3724\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3950 - val_loss: 0.3698\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3912 - val_loss: 0.3670\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.3662\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3868 - val_loss: 0.3631\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3878 - val_loss: 0.3663\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3936 - val_loss: 0.3628\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3817 - val_loss: 0.3593\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3801 - val_loss: 0.3563\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3679 - val_loss: 0.3535\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3624 - val_loss: 0.3712\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3746 - val_loss: 0.3512\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3605 - val_loss: 0.3704\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3822 - val_loss: 0.3477\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3626 - val_loss: 0.3565\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3610 - val_loss: 0.3530\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3626 - val_loss: 0.3701\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3685 - val_loss: 0.3432\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3684 - val_loss: 0.3590\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3581 - val_loss: 0.3524\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3687 - val_loss: 0.3629\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3613 - val_loss: 0.3431\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.3765\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3620 - val_loss: 0.3373\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3502 - val_loss: 0.3407\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3471 - val_loss: 0.3613\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3451 - val_loss: 0.3348\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3780 - val_loss: 0.3576\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3474 - val_loss: 0.3366\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3689 - val_loss: 0.3428\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3485 - val_loss: 0.3370\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3675 - val_loss: 0.3513\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3471 - val_loss: 0.3423\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3545 - val_loss: 0.3682\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3407 - val_loss: 0.3563\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3554 - val_loss: 0.3337\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3499 - val_loss: 0.3457\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3623 - val_loss: 0.3436\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3401 - val_loss: 0.3652\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3528 - val_loss: 0.3287\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3560 - val_loss: 0.3268\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3483 - val_loss: 0.3439\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3405 - val_loss: 0.3263\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3468 - val_loss: 0.3919\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3337 - val_loss: 0.3273\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3463 - val_loss: 0.3553\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3342 - val_loss: 0.3237\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3395 - val_loss: 0.3243\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3314 - val_loss: 0.3766\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3395 - val_loss: 0.3287\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3378 - val_loss: 0.3501\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3522 - val_loss: 0.3457\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3473 - val_loss: 0.3438\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3427 - val_loss: 0.3289\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3212 - val_loss: 0.3218\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3375 - val_loss: 0.3350\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3323 - val_loss: 0.3232\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3470 - val_loss: 0.3557\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3316 - val_loss: 0.3257\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3354 - val_loss: 0.3345\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3316 - val_loss: 0.3563\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3371 - val_loss: 0.3583\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3201 - val_loss: 0.3287\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3373 - val_loss: 0.3202\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3328 - val_loss: 0.3840\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3268 - val_loss: 0.3234\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3322 - val_loss: 0.3478\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3224 - val_loss: 0.3411\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3332 - val_loss: 0.3462\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3310 - val_loss: 0.3350\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3323 - val_loss: 0.3351\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3297 - val_loss: 0.3275\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3441 - val_loss: 0.3168\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3369 - val_loss: 0.3281\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3183 - val_loss: 0.3633\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3235 - val_loss: 0.3176\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3185 - val_loss: 0.3156\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3394 - val_loss: 0.3534\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3264 - val_loss: 0.3256\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3210 - val_loss: 0.3630\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3192 - val_loss: 0.3378\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3237 - val_loss: 0.3212\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3281 - val_loss: 0.3465\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3424 - val_loss: 0.3158\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3209 - val_loss: 0.3406\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3230 - val_loss: 0.3379\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3342 - val_loss: 0.3209\n",
      "162/162 [==============================] - 0s 880us/step - loss: 0.3310\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model.compile(loss=\"mse\", \n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义回调函数\n",
    "\n",
    "- 自定义回调通过继承`keras.callbacks.Callback`实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T23:18:24.944550Z",
     "start_time": "2021-03-14T23:18:23.973395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Start epoch 0 of training\n",
      "...Training: start of batch 0\n",
      "  1/363 [..............................] - ETA: 1s - loss: 0.5215...Training: end of batch 0; loss: 0.5215221643447876\n",
      "...Training: start of batch 1\n",
      "...Training: end of batch 1; loss: 0.38260430097579956\n",
      "...Training: start of batch 2\n",
      "...Training: end of batch 2; loss: 0.41070976853370667\n",
      "...Training: start of batch 3\n",
      "...Training: end of batch 3; loss: 0.39290082454681396\n",
      "...Training: start of batch 4\n",
      "...Training: end of batch 4; loss: 0.39595308899879456\n",
      "...Training: start of batch 5\n",
      "...Training: end of batch 5; loss: 0.404376357793808\n",
      "...Training: start of batch 6\n",
      "...Training: end of batch 6; loss: 0.38314396142959595\n",
      "...Training: start of batch 7\n",
      "...Training: end of batch 7; loss: 0.36870065331459045\n",
      "...Training: start of batch 8\n",
      "...Training: end of batch 8; loss: 0.3704698383808136\n",
      "...Training: start of batch 9\n",
      "...Training: end of batch 9; loss: 0.374715656042099\n",
      "...Training: start of batch 10\n",
      "...Training: end of batch 10; loss: 0.37105971574783325\n",
      "...Training: start of batch 11\n",
      "...Training: end of batch 11; loss: 0.37247470021247864\n",
      "...Training: start of batch 12\n",
      "...Training: end of batch 12; loss: 0.3586767613887787\n",
      "...Training: start of batch 13\n",
      "...Training: end of batch 13; loss: 0.37469059228897095\n",
      "...Training: start of batch 14\n",
      "...Training: end of batch 14; loss: 0.3730632960796356\n",
      "...Training: start of batch 15\n",
      "...Training: end of batch 15; loss: 0.3878960609436035\n",
      "...Training: start of batch 16\n",
      "...Training: end of batch 16; loss: 0.39729174971580505\n",
      "...Training: start of batch 17\n",
      "...Training: end of batch 17; loss: 0.3942817151546478\n",
      "...Training: start of batch 18\n",
      "...Training: end of batch 18; loss: 0.40672212839126587\n",
      "...Training: start of batch 19\n",
      "...Training: end of batch 19; loss: 0.39489492774009705\n",
      "...Training: start of batch 20\n",
      "...Training: end of batch 20; loss: 0.38954317569732666\n",
      "...Training: start of batch 21\n",
      "...Training: end of batch 21; loss: 0.39072301983833313\n",
      "...Training: start of batch 22\n",
      "...Training: end of batch 22; loss: 0.3860595226287842\n",
      "...Training: start of batch 23\n",
      "...Training: end of batch 23; loss: 0.3874673545360565\n",
      "...Training: start of batch 24\n",
      "...Training: end of batch 24; loss: 0.3881567120552063\n",
      "...Training: start of batch 25\n",
      "...Training: end of batch 25; loss: 0.39432746171951294\n",
      "...Training: start of batch 26\n",
      "...Training: end of batch 26; loss: 0.3888128399848938\n",
      "...Training: start of batch 27\n",
      "...Training: end of batch 27; loss: 0.38006362318992615\n",
      "...Training: start of batch 28\n",
      "...Training: end of batch 28; loss: 0.3832587003707886\n",
      "...Training: start of batch 29\n",
      " 30/363 [=>............................] - ETA: 0s - loss: 0.3765...Training: end of batch 29; loss: 0.37654027342796326\n",
      "...Training: start of batch 30\n",
      "...Training: end of batch 30; loss: 0.3815475404262543\n",
      "...Training: start of batch 31\n",
      "...Training: end of batch 31; loss: 0.37573423981666565\n",
      "...Training: start of batch 32\n",
      "...Training: end of batch 32; loss: 0.3756549656391144\n",
      "...Training: start of batch 33\n",
      "...Training: end of batch 33; loss: 0.3700931668281555\n",
      "...Training: start of batch 34\n",
      "...Training: end of batch 34; loss: 0.3630765974521637\n",
      "...Training: start of batch 35\n",
      "...Training: end of batch 35; loss: 0.35862934589385986\n",
      "...Training: start of batch 36\n",
      "...Training: end of batch 36; loss: 0.356701523065567\n",
      "...Training: start of batch 37\n",
      "...Training: end of batch 37; loss: 0.354181706905365\n",
      "...Training: start of batch 38\n",
      "...Training: end of batch 38; loss: 0.3511865437030792\n",
      "...Training: start of batch 39\n",
      "...Training: end of batch 39; loss: 0.35747724771499634\n",
      "...Training: start of batch 40\n",
      "...Training: end of batch 40; loss: 0.3523712754249573\n",
      "...Training: start of batch 41\n",
      "...Training: end of batch 41; loss: 0.3489088714122772\n",
      "...Training: start of batch 42\n",
      "...Training: end of batch 42; loss: 0.3491390645503998\n",
      "...Training: start of batch 43\n",
      "...Training: end of batch 43; loss: 0.34444352984428406\n",
      "...Training: start of batch 44\n",
      "...Training: end of batch 44; loss: 0.34328925609588623\n",
      "...Training: start of batch 45\n",
      "...Training: end of batch 45; loss: 0.34123528003692627\n",
      "...Training: start of batch 46\n",
      "...Training: end of batch 46; loss: 0.3414028584957123\n",
      "...Training: start of batch 47\n",
      "...Training: end of batch 47; loss: 0.3439638614654541\n",
      "...Training: start of batch 48\n",
      "...Training: end of batch 48; loss: 0.3486584722995758\n",
      "...Training: start of batch 49\n",
      "...Training: end of batch 49; loss: 0.346062570810318\n",
      "...Training: start of batch 50\n",
      "...Training: end of batch 50; loss: 0.3433220386505127\n",
      "...Training: start of batch 51\n",
      "...Training: end of batch 51; loss: 0.3408587872982025\n",
      "...Training: start of batch 52\n",
      "...Training: end of batch 52; loss: 0.3389689326286316\n",
      "...Training: start of batch 53\n",
      "...Training: end of batch 53; loss: 0.3377000689506531\n",
      "...Training: start of batch 54\n",
      "...Training: end of batch 54; loss: 0.3377775251865387\n",
      "...Training: start of batch 55\n",
      "...Training: end of batch 55; loss: 0.3398888409137726\n",
      "...Training: start of batch 56\n",
      "...Training: end of batch 56; loss: 0.34161677956581116\n",
      "...Training: start of batch 57\n",
      "...Training: end of batch 57; loss: 0.3390961289405823\n",
      "...Training: start of batch 58\n",
      " 59/363 [===>..........................] - ETA: 0s - loss: 0.3381...Training: end of batch 58; loss: 0.3381236791610718\n",
      "...Training: start of batch 59\n",
      "...Training: end of batch 59; loss: 0.3360833525657654\n",
      "...Training: start of batch 60\n",
      "...Training: end of batch 60; loss: 0.3357817530632019\n",
      "...Training: start of batch 61\n",
      "...Training: end of batch 61; loss: 0.3390808403491974\n",
      "...Training: start of batch 62\n",
      "...Training: end of batch 62; loss: 0.3362584114074707\n",
      "...Training: start of batch 63\n",
      "...Training: end of batch 63; loss: 0.3339591920375824\n",
      "...Training: start of batch 64\n",
      "...Training: end of batch 64; loss: 0.3325726389884949\n",
      "...Training: start of batch 65\n",
      "...Training: end of batch 65; loss: 0.3337753713130951\n",
      "...Training: start of batch 66\n",
      "...Training: end of batch 66; loss: 0.3331648111343384\n",
      "...Training: start of batch 67\n",
      "...Training: end of batch 67; loss: 0.33397534489631653\n",
      "...Training: start of batch 68\n",
      "...Training: end of batch 68; loss: 0.33561161160469055\n",
      "...Training: start of batch 69\n",
      "...Training: end of batch 69; loss: 0.3345804512500763\n",
      "...Training: start of batch 70\n",
      "...Training: end of batch 70; loss: 0.3333272933959961\n",
      "...Training: start of batch 71\n",
      "...Training: end of batch 71; loss: 0.3345727324485779\n",
      "...Training: start of batch 72\n",
      "...Training: end of batch 72; loss: 0.3353567123413086\n",
      "...Training: start of batch 73\n",
      "...Training: end of batch 73; loss: 0.33355075120925903\n",
      "...Training: start of batch 74\n",
      "...Training: end of batch 74; loss: 0.33603399991989136\n",
      "...Training: start of batch 75\n",
      "...Training: end of batch 75; loss: 0.338700532913208\n",
      "...Training: start of batch 76\n",
      "...Training: end of batch 76; loss: 0.34102681279182434\n",
      "...Training: start of batch 77\n",
      "...Training: end of batch 77; loss: 0.3380483090877533\n",
      "...Training: start of batch 78\n",
      "...Training: end of batch 78; loss: 0.3420764207839966\n",
      "...Training: start of batch 79\n",
      "...Training: end of batch 79; loss: 0.34008651971817017\n",
      "...Training: start of batch 80\n",
      "...Training: end of batch 80; loss: 0.3411596417427063\n",
      "...Training: start of batch 81\n",
      "...Training: end of batch 81; loss: 0.3398400843143463\n",
      "...Training: start of batch 82\n",
      "...Training: end of batch 82; loss: 0.3378250002861023\n",
      "...Training: start of batch 83\n",
      "...Training: end of batch 83; loss: 0.33991485834121704\n",
      "...Training: start of batch 84\n",
      "...Training: end of batch 84; loss: 0.3379174768924713\n",
      "...Training: start of batch 85\n",
      "...Training: end of batch 85; loss: 0.3376254737377167\n",
      "...Training: start of batch 86\n",
      " 87/363 [======>.......................] - ETA: 0s - loss: 0.3373...Training: end of batch 86; loss: 0.3372669816017151\n",
      "...Training: start of batch 87\n",
      "...Training: end of batch 87; loss: 0.3362559974193573\n",
      "...Training: start of batch 88\n",
      "...Training: end of batch 88; loss: 0.34078022837638855\n",
      "...Training: start of batch 89\n",
      "...Training: end of batch 89; loss: 0.3388356864452362\n",
      "...Training: start of batch 90\n",
      "...Training: end of batch 90; loss: 0.3377525508403778\n",
      "...Training: start of batch 91\n",
      "...Training: end of batch 91; loss: 0.3365141451358795\n",
      "...Training: start of batch 92\n",
      "...Training: end of batch 92; loss: 0.33512845635414124\n",
      "...Training: start of batch 93\n",
      "...Training: end of batch 93; loss: 0.3357936441898346\n",
      "...Training: start of batch 94\n",
      "...Training: end of batch 94; loss: 0.3337257504463196\n",
      "...Training: start of batch 95\n",
      "...Training: end of batch 95; loss: 0.3347140848636627\n",
      "...Training: start of batch 96\n",
      "...Training: end of batch 96; loss: 0.3325234353542328\n",
      "...Training: start of batch 97\n",
      "...Training: end of batch 97; loss: 0.330801784992218\n",
      "...Training: start of batch 98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 98; loss: 0.33025291562080383\n",
      "...Training: start of batch 99\n",
      "...Training: end of batch 99; loss: 0.33085164427757263\n",
      "...Training: start of batch 100\n",
      "...Training: end of batch 100; loss: 0.3292800486087799\n",
      "...Training: start of batch 101\n",
      "...Training: end of batch 101; loss: 0.3296352028846741\n",
      "...Training: start of batch 102\n",
      "...Training: end of batch 102; loss: 0.3301103413105011\n",
      "...Training: start of batch 103\n",
      "...Training: end of batch 103; loss: 0.3301202058792114\n",
      "...Training: start of batch 104\n",
      "...Training: end of batch 104; loss: 0.3298487663269043\n",
      "...Training: start of batch 105\n",
      "...Training: end of batch 105; loss: 0.3308788239955902\n",
      "...Training: start of batch 106\n",
      "...Training: end of batch 106; loss: 0.329477459192276\n",
      "...Training: start of batch 107\n",
      "...Training: end of batch 107; loss: 0.32812774181365967\n",
      "...Training: start of batch 108\n",
      "...Training: end of batch 108; loss: 0.32802730798721313\n",
      "...Training: start of batch 109\n",
      "...Training: end of batch 109; loss: 0.32800227403640747\n",
      "...Training: start of batch 110\n",
      "...Training: end of batch 110; loss: 0.3296748101711273\n",
      "...Training: start of batch 111\n",
      "...Training: end of batch 111; loss: 0.3296346962451935\n",
      "...Training: start of batch 112\n",
      "...Training: end of batch 112; loss: 0.3323918581008911\n",
      "...Training: start of batch 113\n",
      "...Training: end of batch 113; loss: 0.33120378851890564\n",
      "...Training: start of batch 114\n",
      "...Training: end of batch 114; loss: 0.3295472264289856\n",
      "...Training: start of batch 115\n",
      "116/363 [========>.....................] - ETA: 0s - loss: 0.3302...Training: end of batch 115; loss: 0.33021295070648193\n",
      "...Training: start of batch 116\n",
      "...Training: end of batch 116; loss: 0.3296109735965729\n",
      "...Training: start of batch 117\n",
      "...Training: end of batch 117; loss: 0.3325102627277374\n",
      "...Training: start of batch 118\n",
      "...Training: end of batch 118; loss: 0.3312191963195801\n",
      "...Training: start of batch 119\n",
      "...Training: end of batch 119; loss: 0.3324567675590515\n",
      "...Training: start of batch 120\n",
      "...Training: end of batch 120; loss: 0.3335627317428589\n",
      "...Training: start of batch 121\n",
      "...Training: end of batch 121; loss: 0.3336653709411621\n",
      "...Training: start of batch 122\n",
      "...Training: end of batch 122; loss: 0.33806970715522766\n",
      "...Training: start of batch 123\n",
      "...Training: end of batch 123; loss: 0.3375677168369293\n",
      "...Training: start of batch 124\n",
      "...Training: end of batch 124; loss: 0.3364450931549072\n",
      "...Training: start of batch 125\n",
      "...Training: end of batch 125; loss: 0.33845072984695435\n",
      "...Training: start of batch 126\n",
      "...Training: end of batch 126; loss: 0.33943793177604675\n",
      "...Training: start of batch 127\n",
      "...Training: end of batch 127; loss: 0.34184688329696655\n",
      "...Training: start of batch 128\n",
      "...Training: end of batch 128; loss: 0.341503769159317\n",
      "...Training: start of batch 129\n",
      "...Training: end of batch 129; loss: 0.34085899591445923\n",
      "...Training: start of batch 130\n",
      "...Training: end of batch 130; loss: 0.34078288078308105\n",
      "...Training: start of batch 131\n",
      "...Training: end of batch 131; loss: 0.3404942452907562\n",
      "...Training: start of batch 132\n",
      "...Training: end of batch 132; loss: 0.3393639326095581\n",
      "...Training: start of batch 133\n",
      "...Training: end of batch 133; loss: 0.33911216259002686\n",
      "...Training: start of batch 134\n",
      "...Training: end of batch 134; loss: 0.33942702412605286\n",
      "...Training: start of batch 135\n",
      "...Training: end of batch 135; loss: 0.3389293849468231\n",
      "...Training: start of batch 136\n",
      "...Training: end of batch 136; loss: 0.33817458152770996\n",
      "...Training: start of batch 137\n",
      "...Training: end of batch 137; loss: 0.3404562473297119\n",
      "...Training: start of batch 138\n",
      "...Training: end of batch 138; loss: 0.3413625657558441\n",
      "...Training: start of batch 139\n",
      "...Training: end of batch 139; loss: 0.3415975570678711\n",
      "...Training: start of batch 140\n",
      "...Training: end of batch 140; loss: 0.3406354784965515\n",
      "...Training: start of batch 141\n",
      "...Training: end of batch 141; loss: 0.33974218368530273\n",
      "...Training: start of batch 142\n",
      "...Training: end of batch 142; loss: 0.33900219202041626\n",
      "...Training: start of batch 143\n",
      "144/363 [==========>...................] - ETA: 0s - loss: 0.3403...Training: end of batch 143; loss: 0.3403402268886566\n",
      "...Training: start of batch 144\n",
      "...Training: end of batch 144; loss: 0.33992987871170044\n",
      "...Training: start of batch 145\n",
      "...Training: end of batch 145; loss: 0.34012195467948914\n",
      "...Training: start of batch 146\n",
      "...Training: end of batch 146; loss: 0.3397497534751892\n",
      "...Training: start of batch 147\n",
      "...Training: end of batch 147; loss: 0.3388063311576843\n",
      "...Training: start of batch 148\n",
      "...Training: end of batch 148; loss: 0.33944159746170044\n",
      "...Training: start of batch 149\n",
      "...Training: end of batch 149; loss: 0.3387892544269562\n",
      "...Training: start of batch 150\n",
      "...Training: end of batch 150; loss: 0.3382452726364136\n",
      "...Training: start of batch 151\n",
      "...Training: end of batch 151; loss: 0.3373560607433319\n",
      "...Training: start of batch 152\n",
      "...Training: end of batch 152; loss: 0.3385362923145294\n",
      "...Training: start of batch 153\n",
      "...Training: end of batch 153; loss: 0.33752039074897766\n",
      "...Training: start of batch 154\n",
      "...Training: end of batch 154; loss: 0.33724531531333923\n",
      "...Training: start of batch 155\n",
      "...Training: end of batch 155; loss: 0.3368946611881256\n",
      "...Training: start of batch 156\n",
      "...Training: end of batch 156; loss: 0.3363915979862213\n",
      "...Training: start of batch 157\n",
      "...Training: end of batch 157; loss: 0.3360644578933716\n",
      "...Training: start of batch 158\n",
      "...Training: end of batch 158; loss: 0.3352642059326172\n",
      "...Training: start of batch 159\n",
      "...Training: end of batch 159; loss: 0.3360068202018738\n",
      "...Training: start of batch 160\n",
      "...Training: end of batch 160; loss: 0.3349556028842926\n",
      "...Training: start of batch 161\n",
      "...Training: end of batch 161; loss: 0.3376334309577942\n",
      "...Training: start of batch 162\n",
      "...Training: end of batch 162; loss: 0.3366205096244812\n",
      "...Training: start of batch 163\n",
      "...Training: end of batch 163; loss: 0.33612534403800964\n",
      "...Training: start of batch 164\n",
      "...Training: end of batch 164; loss: 0.3360390067100525\n",
      "...Training: start of batch 165\n",
      "...Training: end of batch 165; loss: 0.33742809295654297\n",
      "...Training: start of batch 166\n",
      "...Training: end of batch 166; loss: 0.33664247393608093\n",
      "...Training: start of batch 167\n",
      "...Training: end of batch 167; loss: 0.33561956882476807\n",
      "...Training: start of batch 168\n",
      "...Training: end of batch 168; loss: 0.3346817195415497\n",
      "...Training: start of batch 169\n",
      "...Training: end of batch 169; loss: 0.3352493643760681\n",
      "...Training: start of batch 170\n",
      "...Training: end of batch 170; loss: 0.334920734167099\n",
      "...Training: start of batch 171\n",
      "172/363 [=============>................] - ETA: 0s - loss: 0.3342...Training: end of batch 171; loss: 0.3342219293117523\n",
      "...Training: start of batch 172\n",
      "...Training: end of batch 172; loss: 0.3334669768810272\n",
      "...Training: start of batch 173\n",
      "...Training: end of batch 173; loss: 0.33430567383766174\n",
      "...Training: start of batch 174\n",
      "...Training: end of batch 174; loss: 0.3344154953956604\n",
      "...Training: start of batch 175\n",
      "...Training: end of batch 175; loss: 0.3346615135669708\n",
      "...Training: start of batch 176\n",
      "...Training: end of batch 176; loss: 0.3341936767101288\n",
      "...Training: start of batch 177\n",
      "...Training: end of batch 177; loss: 0.333810955286026\n",
      "...Training: start of batch 178\n",
      "...Training: end of batch 178; loss: 0.3331839144229889\n",
      "...Training: start of batch 179\n",
      "...Training: end of batch 179; loss: 0.3325859606266022\n",
      "...Training: start of batch 180\n",
      "...Training: end of batch 180; loss: 0.3315059542655945\n",
      "...Training: start of batch 181\n",
      "...Training: end of batch 181; loss: 0.33139878511428833\n",
      "...Training: start of batch 182\n",
      "...Training: end of batch 182; loss: 0.33212143182754517\n",
      "...Training: start of batch 183\n",
      "...Training: end of batch 183; loss: 0.3311077952384949\n",
      "...Training: start of batch 184\n",
      "...Training: end of batch 184; loss: 0.33275747299194336\n",
      "...Training: start of batch 185\n",
      "...Training: end of batch 185; loss: 0.3318933844566345\n",
      "...Training: start of batch 186\n",
      "...Training: end of batch 186; loss: 0.3313693106174469\n",
      "...Training: start of batch 187\n",
      "...Training: end of batch 187; loss: 0.3326708972454071\n",
      "...Training: start of batch 188\n",
      "...Training: end of batch 188; loss: 0.3322499096393585\n",
      "...Training: start of batch 189\n",
      "...Training: end of batch 189; loss: 0.33207952976226807\n",
      "...Training: start of batch 190\n",
      "...Training: end of batch 190; loss: 0.3325404226779938\n",
      "...Training: start of batch 191\n",
      "...Training: end of batch 191; loss: 0.33383575081825256\n",
      "...Training: start of batch 192\n",
      "...Training: end of batch 192; loss: 0.33352193236351013\n",
      "...Training: start of batch 193\n",
      "...Training: end of batch 193; loss: 0.33318018913269043\n",
      "...Training: start of batch 194\n",
      "...Training: end of batch 194; loss: 0.33369481563568115\n",
      "...Training: start of batch 195\n",
      "...Training: end of batch 195; loss: 0.33376455307006836\n",
      "...Training: start of batch 196\n",
      "...Training: end of batch 196; loss: 0.33374109864234924\n",
      "...Training: start of batch 197\n",
      "...Training: end of batch 197; loss: 0.33365339040756226\n",
      "...Training: start of batch 198\n",
      "...Training: end of batch 198; loss: 0.3331024646759033\n",
      "...Training: start of batch 199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/363 [===============>..............] - ETA: 0s - loss: 0.3324...Training: end of batch 199; loss: 0.3323827385902405\n",
      "...Training: start of batch 200\n",
      "...Training: end of batch 200; loss: 0.3336067795753479\n",
      "...Training: start of batch 201\n",
      "...Training: end of batch 201; loss: 0.33309072256088257\n",
      "...Training: start of batch 202\n",
      "...Training: end of batch 202; loss: 0.3329603970050812\n",
      "...Training: start of batch 203\n",
      "...Training: end of batch 203; loss: 0.33207282423973083\n",
      "...Training: start of batch 204\n",
      "...Training: end of batch 204; loss: 0.331658273935318\n",
      "...Training: start of batch 205\n",
      "...Training: end of batch 205; loss: 0.3310810327529907\n",
      "...Training: start of batch 206\n",
      "...Training: end of batch 206; loss: 0.3315809369087219\n",
      "...Training: start of batch 207\n",
      "...Training: end of batch 207; loss: 0.3312411904335022\n",
      "...Training: start of batch 208\n",
      "...Training: end of batch 208; loss: 0.3308991491794586\n",
      "...Training: start of batch 209\n",
      "...Training: end of batch 209; loss: 0.3312266170978546\n",
      "...Training: start of batch 210\n",
      "...Training: end of batch 210; loss: 0.3312881290912628\n",
      "...Training: start of batch 211\n",
      "...Training: end of batch 211; loss: 0.33069750666618347\n",
      "...Training: start of batch 212\n",
      "...Training: end of batch 212; loss: 0.3316546082496643\n",
      "...Training: start of batch 213\n",
      "...Training: end of batch 213; loss: 0.3317522406578064\n",
      "...Training: start of batch 214\n",
      "...Training: end of batch 214; loss: 0.3310505449771881\n",
      "...Training: start of batch 215\n",
      "...Training: end of batch 215; loss: 0.3314630091190338\n",
      "...Training: start of batch 216\n",
      "...Training: end of batch 216; loss: 0.33124563097953796\n",
      "...Training: start of batch 217\n",
      "...Training: end of batch 217; loss: 0.3309410810470581\n",
      "...Training: start of batch 218\n",
      "...Training: end of batch 218; loss: 0.3299810588359833\n",
      "...Training: start of batch 219\n",
      "...Training: end of batch 219; loss: 0.32940906286239624\n",
      "...Training: start of batch 220\n",
      "...Training: end of batch 220; loss: 0.3306490480899811\n",
      "...Training: start of batch 221\n",
      "...Training: end of batch 221; loss: 0.32985061407089233\n",
      "...Training: start of batch 222\n",
      "...Training: end of batch 222; loss: 0.32956385612487793\n",
      "...Training: start of batch 223\n",
      "...Training: end of batch 223; loss: 0.33021092414855957\n",
      "...Training: start of batch 224\n",
      "...Training: end of batch 224; loss: 0.3295745849609375\n",
      "...Training: start of batch 225\n",
      "...Training: end of batch 225; loss: 0.3290530741214752\n",
      "...Training: start of batch 226\n",
      "227/363 [=================>............] - ETA: 0s - loss: 0.3293...Training: end of batch 226; loss: 0.32929667830467224\n",
      "...Training: start of batch 227\n",
      "...Training: end of batch 227; loss: 0.32902613282203674\n",
      "...Training: start of batch 228\n",
      "...Training: end of batch 228; loss: 0.328624427318573\n",
      "...Training: start of batch 229\n",
      "...Training: end of batch 229; loss: 0.3282662034034729\n",
      "...Training: start of batch 230\n",
      "...Training: end of batch 230; loss: 0.33048927783966064\n",
      "...Training: start of batch 231\n",
      "...Training: end of batch 231; loss: 0.330140620470047\n",
      "...Training: start of batch 232\n",
      "...Training: end of batch 232; loss: 0.3294336795806885\n",
      "...Training: start of batch 233\n",
      "...Training: end of batch 233; loss: 0.3285254240036011\n",
      "...Training: start of batch 234\n",
      "...Training: end of batch 234; loss: 0.32786133885383606\n",
      "...Training: start of batch 235\n",
      "...Training: end of batch 235; loss: 0.3292497396469116\n",
      "...Training: start of batch 236\n",
      "...Training: end of batch 236; loss: 0.32878780364990234\n",
      "...Training: start of batch 237\n",
      "...Training: end of batch 237; loss: 0.32812970876693726\n",
      "...Training: start of batch 238\n",
      "...Training: end of batch 238; loss: 0.32719922065734863\n",
      "...Training: start of batch 239\n",
      "...Training: end of batch 239; loss: 0.3276125490665436\n",
      "...Training: start of batch 240\n",
      "...Training: end of batch 240; loss: 0.32760974764823914\n",
      "...Training: start of batch 241\n",
      "...Training: end of batch 241; loss: 0.32793793082237244\n",
      "...Training: start of batch 242\n",
      "...Training: end of batch 242; loss: 0.328058660030365\n",
      "...Training: start of batch 243\n",
      "...Training: end of batch 243; loss: 0.3286095857620239\n",
      "...Training: start of batch 244\n",
      "...Training: end of batch 244; loss: 0.32886552810668945\n",
      "...Training: start of batch 245\n",
      "...Training: end of batch 245; loss: 0.32967662811279297\n",
      "...Training: start of batch 246\n",
      "...Training: end of batch 246; loss: 0.32971373200416565\n",
      "...Training: start of batch 247\n",
      "...Training: end of batch 247; loss: 0.3292458653450012\n",
      "...Training: start of batch 248\n",
      "...Training: end of batch 248; loss: 0.32928940653800964\n",
      "...Training: start of batch 249\n",
      "...Training: end of batch 249; loss: 0.32936277985572815\n",
      "...Training: start of batch 250\n",
      "...Training: end of batch 250; loss: 0.32937729358673096\n",
      "...Training: start of batch 251\n",
      "...Training: end of batch 251; loss: 0.3301125764846802\n",
      "...Training: start of batch 252\n",
      "...Training: end of batch 252; loss: 0.32985571026802063\n",
      "...Training: start of batch 253\n",
      "254/363 [===================>..........] - ETA: 0s - loss: 0.3320...Training: end of batch 253; loss: 0.3320428729057312\n",
      "...Training: start of batch 254\n",
      "...Training: end of batch 254; loss: 0.33233386278152466\n",
      "...Training: start of batch 255\n",
      "...Training: end of batch 255; loss: 0.3323003649711609\n",
      "...Training: start of batch 256\n",
      "...Training: end of batch 256; loss: 0.33411508798599243\n",
      "...Training: start of batch 257\n",
      "...Training: end of batch 257; loss: 0.333805650472641\n",
      "...Training: start of batch 258\n",
      "...Training: end of batch 258; loss: 0.33403047919273376\n",
      "...Training: start of batch 259\n",
      "...Training: end of batch 259; loss: 0.33404821157455444\n",
      "...Training: start of batch 260\n",
      "...Training: end of batch 260; loss: 0.33376064896583557\n",
      "...Training: start of batch 261\n",
      "...Training: end of batch 261; loss: 0.33382293581962585\n",
      "...Training: start of batch 262\n",
      "...Training: end of batch 262; loss: 0.33382830023765564\n",
      "...Training: start of batch 263\n",
      "...Training: end of batch 263; loss: 0.33367809653282166\n",
      "...Training: start of batch 264\n",
      "...Training: end of batch 264; loss: 0.3339007496833801\n",
      "...Training: start of batch 265\n",
      "...Training: end of batch 265; loss: 0.33354705572128296\n",
      "...Training: start of batch 266\n",
      "...Training: end of batch 266; loss: 0.332811564207077\n",
      "...Training: start of batch 267\n",
      "...Training: end of batch 267; loss: 0.33232784271240234\n",
      "...Training: start of batch 268\n",
      "...Training: end of batch 268; loss: 0.3319626450538635\n",
      "...Training: start of batch 269\n",
      "...Training: end of batch 269; loss: 0.3316785395145416\n",
      "...Training: start of batch 270\n",
      "...Training: end of batch 270; loss: 0.3321011960506439\n",
      "...Training: start of batch 271\n",
      "...Training: end of batch 271; loss: 0.33224692940711975\n",
      "...Training: start of batch 272\n",
      "...Training: end of batch 272; loss: 0.33296340703964233\n",
      "...Training: start of batch 273\n",
      "...Training: end of batch 273; loss: 0.33285054564476013\n",
      "...Training: start of batch 274\n",
      "...Training: end of batch 274; loss: 0.33257633447647095\n",
      "...Training: start of batch 275\n",
      "...Training: end of batch 275; loss: 0.33256131410598755\n",
      "...Training: start of batch 276\n",
      "...Training: end of batch 276; loss: 0.33262908458709717\n",
      "...Training: start of batch 277\n",
      "...Training: end of batch 277; loss: 0.3329423666000366\n",
      "...Training: start of batch 278\n",
      "...Training: end of batch 278; loss: 0.3329157531261444\n",
      "...Training: start of batch 279\n",
      "...Training: end of batch 279; loss: 0.33222392201423645\n",
      "...Training: start of batch 280\n",
      "281/363 [======================>.......] - ETA: 0s - loss: 0.3332...Training: end of batch 280; loss: 0.3331644833087921\n",
      "...Training: start of batch 281\n",
      "...Training: end of batch 281; loss: 0.3335152268409729\n",
      "...Training: start of batch 282\n",
      "...Training: end of batch 282; loss: 0.33298832178115845\n",
      "...Training: start of batch 283\n",
      "...Training: end of batch 283; loss: 0.3325956165790558\n",
      "...Training: start of batch 284\n",
      "...Training: end of batch 284; loss: 0.3319878578186035\n",
      "...Training: start of batch 285\n",
      "...Training: end of batch 285; loss: 0.3319443464279175\n",
      "...Training: start of batch 286\n",
      "...Training: end of batch 286; loss: 0.33133405447006226\n",
      "...Training: start of batch 287\n",
      "...Training: end of batch 287; loss: 0.3311716318130493\n",
      "...Training: start of batch 288\n",
      "...Training: end of batch 288; loss: 0.3307516574859619\n",
      "...Training: start of batch 289\n",
      "...Training: end of batch 289; loss: 0.330574631690979\n",
      "...Training: start of batch 290\n",
      "...Training: end of batch 290; loss: 0.33063557744026184\n",
      "...Training: start of batch 291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 291; loss: 0.33016228675842285\n",
      "...Training: start of batch 292\n",
      "...Training: end of batch 292; loss: 0.329924076795578\n",
      "...Training: start of batch 293\n",
      "...Training: end of batch 293; loss: 0.3297005593776703\n",
      "...Training: start of batch 294\n",
      "...Training: end of batch 294; loss: 0.3293306231498718\n",
      "...Training: start of batch 295\n",
      "...Training: end of batch 295; loss: 0.3289760649204254\n",
      "...Training: start of batch 296\n",
      "...Training: end of batch 296; loss: 0.32887816429138184\n",
      "...Training: start of batch 297\n",
      "...Training: end of batch 297; loss: 0.3283041715621948\n",
      "...Training: start of batch 298\n",
      "...Training: end of batch 298; loss: 0.3279137909412384\n",
      "...Training: start of batch 299\n",
      "...Training: end of batch 299; loss: 0.32732394337654114\n",
      "...Training: start of batch 300\n",
      "...Training: end of batch 300; loss: 0.32727906107902527\n",
      "...Training: start of batch 301\n",
      "...Training: end of batch 301; loss: 0.32672950625419617\n",
      "...Training: start of batch 302\n",
      "...Training: end of batch 302; loss: 0.32679998874664307\n",
      "...Training: start of batch 303\n",
      "...Training: end of batch 303; loss: 0.32652372121810913\n",
      "...Training: start of batch 304\n",
      "...Training: end of batch 304; loss: 0.3262718915939331\n",
      "...Training: start of batch 305\n",
      "...Training: end of batch 305; loss: 0.32606062293052673\n",
      "...Training: start of batch 306\n",
      "...Training: end of batch 306; loss: 0.3261791467666626\n",
      "...Training: start of batch 307\n",
      "...Training: end of batch 307; loss: 0.3260985016822815\n",
      "...Training: start of batch 308\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3259...Training: end of batch 308; loss: 0.3258529007434845\n",
      "...Training: start of batch 309\n",
      "...Training: end of batch 309; loss: 0.3254484534263611\n",
      "...Training: start of batch 310\n",
      "...Training: end of batch 310; loss: 0.3253788352012634\n",
      "...Training: start of batch 311\n",
      "...Training: end of batch 311; loss: 0.32530203461647034\n",
      "...Training: start of batch 312\n",
      "...Training: end of batch 312; loss: 0.32522428035736084\n",
      "...Training: start of batch 313\n",
      "...Training: end of batch 313; loss: 0.32548463344573975\n",
      "...Training: start of batch 314\n",
      "...Training: end of batch 314; loss: 0.3262116312980652\n",
      "...Training: start of batch 315\n",
      "...Training: end of batch 315; loss: 0.3266245722770691\n",
      "...Training: start of batch 316\n",
      "...Training: end of batch 316; loss: 0.32614248991012573\n",
      "...Training: start of batch 317\n",
      "...Training: end of batch 317; loss: 0.3258962035179138\n",
      "...Training: start of batch 318\n",
      "...Training: end of batch 318; loss: 0.3265049457550049\n",
      "...Training: start of batch 319\n",
      "...Training: end of batch 319; loss: 0.32585716247558594\n",
      "...Training: start of batch 320\n",
      "...Training: end of batch 320; loss: 0.32552504539489746\n",
      "...Training: start of batch 321\n",
      "...Training: end of batch 321; loss: 0.32645949721336365\n",
      "...Training: start of batch 322\n",
      "...Training: end of batch 322; loss: 0.32680028676986694\n",
      "...Training: start of batch 323\n",
      "...Training: end of batch 323; loss: 0.32655251026153564\n",
      "...Training: start of batch 324\n",
      "...Training: end of batch 324; loss: 0.32677537202835083\n",
      "...Training: start of batch 325\n",
      "...Training: end of batch 325; loss: 0.3261168897151947\n",
      "...Training: start of batch 326\n",
      "...Training: end of batch 326; loss: 0.3257846534252167\n",
      "...Training: start of batch 327\n",
      "...Training: end of batch 327; loss: 0.3257306218147278\n",
      "...Training: start of batch 328\n",
      "...Training: end of batch 328; loss: 0.3252483606338501\n",
      "...Training: start of batch 329\n",
      "...Training: end of batch 329; loss: 0.32551929354667664\n",
      "...Training: start of batch 330\n",
      "...Training: end of batch 330; loss: 0.32519200444221497\n",
      "...Training: start of batch 331\n",
      "...Training: end of batch 331; loss: 0.32535165548324585\n",
      "...Training: start of batch 332\n",
      "...Training: end of batch 332; loss: 0.32514193654060364\n",
      "...Training: start of batch 333\n",
      "...Training: end of batch 333; loss: 0.32485276460647583\n",
      "...Training: start of batch 334\n",
      "335/363 [==========================>...] - ETA: 0s - loss: 0.3253...Training: end of batch 334; loss: 0.3252769708633423\n",
      "...Training: start of batch 335\n",
      "...Training: end of batch 335; loss: 0.32630977034568787\n",
      "...Training: start of batch 336\n",
      "...Training: end of batch 336; loss: 0.32590630650520325\n",
      "...Training: start of batch 337\n",
      "...Training: end of batch 337; loss: 0.32534095644950867\n",
      "...Training: start of batch 338\n",
      "...Training: end of batch 338; loss: 0.3253020942211151\n",
      "...Training: start of batch 339\n",
      "...Training: end of batch 339; loss: 0.3248446583747864\n",
      "...Training: start of batch 340\n",
      "...Training: end of batch 340; loss: 0.3245755136013031\n",
      "...Training: start of batch 341\n",
      "...Training: end of batch 341; loss: 0.3244347870349884\n",
      "...Training: start of batch 342\n",
      "...Training: end of batch 342; loss: 0.32454243302345276\n",
      "...Training: start of batch 343\n",
      "...Training: end of batch 343; loss: 0.32467392086982727\n",
      "...Training: start of batch 344\n",
      "...Training: end of batch 344; loss: 0.3243500888347626\n",
      "...Training: start of batch 345\n",
      "...Training: end of batch 345; loss: 0.32403555512428284\n",
      "...Training: start of batch 346\n",
      "...Training: end of batch 346; loss: 0.3237868547439575\n",
      "...Training: start of batch 347\n",
      "...Training: end of batch 347; loss: 0.3251270055770874\n",
      "...Training: start of batch 348\n",
      "...Training: end of batch 348; loss: 0.325639009475708\n",
      "...Training: start of batch 349\n",
      "...Training: end of batch 349; loss: 0.32619261741638184\n",
      "...Training: start of batch 350\n",
      "...Training: end of batch 350; loss: 0.3258870542049408\n",
      "...Training: start of batch 351\n",
      "...Training: end of batch 351; loss: 0.3260937035083771\n",
      "...Training: start of batch 352\n",
      "...Training: end of batch 352; loss: 0.32573625445365906\n",
      "...Training: start of batch 353\n",
      "...Training: end of batch 353; loss: 0.32546132802963257\n",
      "...Training: start of batch 354\n",
      "...Training: end of batch 354; loss: 0.326698899269104\n",
      "...Training: start of batch 355\n",
      "...Training: end of batch 355; loss: 0.32648995518684387\n",
      "...Training: start of batch 356\n",
      "...Training: end of batch 356; loss: 0.32617107033729553\n",
      "...Training: start of batch 357\n",
      "...Training: end of batch 357; loss: 0.32651135325431824\n",
      "...Training: start of batch 358\n",
      "...Training: end of batch 358; loss: 0.32620710134506226\n",
      "...Training: start of batch 359\n",
      "...Training: end of batch 359; loss: 0.3264634311199188\n",
      "...Training: start of batch 360\n",
      "...Training: end of batch 360; loss: 0.32635697722435\n",
      "...Training: start of batch 361\n",
      "362/363 [============================>.] - ETA: 0s - loss: 0.3263...Training: end of batch 361; loss: 0.3263055086135864\n",
      "...Training: start of batch 362\n",
      "...Training: end of batch 362; loss: 0.3269706964492798\n",
      "Start testing\n",
      "...Evaluating: start of batch 0\n",
      "...Evaluating: end of batch 0; loss: 0.25409597158432007\n",
      "...Evaluating: start of batch 1\n",
      "...Evaluating: end of batch 1; loss: 0.23852765560150146\n",
      "...Evaluating: start of batch 2\n",
      "...Evaluating: end of batch 2; loss: 0.2660839259624481\n",
      "...Evaluating: start of batch 3\n",
      "...Evaluating: end of batch 3; loss: 0.29862815141677856\n",
      "...Evaluating: start of batch 4\n",
      "...Evaluating: end of batch 4; loss: 0.31112343072891235\n",
      "...Evaluating: start of batch 5\n",
      "...Evaluating: end of batch 5; loss: 0.28020280599594116\n",
      "...Evaluating: start of batch 6\n",
      "...Evaluating: end of batch 6; loss: 0.2940635085105896\n",
      "...Evaluating: start of batch 7\n",
      "...Evaluating: end of batch 7; loss: 0.2800268530845642\n",
      "...Evaluating: start of batch 8\n",
      "...Evaluating: end of batch 8; loss: 0.30833640694618225\n",
      "...Evaluating: start of batch 9\n",
      "...Evaluating: end of batch 9; loss: 0.2923280894756317\n",
      "...Evaluating: start of batch 10\n",
      "...Evaluating: end of batch 10; loss: 0.30703383684158325\n",
      "...Evaluating: start of batch 11\n",
      "...Evaluating: end of batch 11; loss: 0.3046140968799591\n",
      "...Evaluating: start of batch 12\n",
      "...Evaluating: end of batch 12; loss: 0.293197363615036\n",
      "...Evaluating: start of batch 13\n",
      "...Evaluating: end of batch 13; loss: 0.3076685965061188\n",
      "...Evaluating: start of batch 14\n",
      "...Evaluating: end of batch 14; loss: 0.310479998588562\n",
      "...Evaluating: start of batch 15\n",
      "...Evaluating: end of batch 15; loss: 0.33040255308151245\n",
      "...Evaluating: start of batch 16\n",
      "...Evaluating: end of batch 16; loss: 0.32907137274742126\n",
      "...Evaluating: start of batch 17\n",
      "...Evaluating: end of batch 17; loss: 0.3183809220790863\n",
      "...Evaluating: start of batch 18\n",
      "...Evaluating: end of batch 18; loss: 0.3203755021095276\n",
      "...Evaluating: start of batch 19\n",
      "...Evaluating: end of batch 19; loss: 0.3206321597099304\n",
      "...Evaluating: start of batch 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Evaluating: end of batch 20; loss: 0.32957571744918823\n",
      "...Evaluating: start of batch 21\n",
      "...Evaluating: end of batch 21; loss: 0.32394033670425415\n",
      "...Evaluating: start of batch 22\n",
      "...Evaluating: end of batch 22; loss: 0.32553213834762573\n",
      "...Evaluating: start of batch 23\n",
      "...Evaluating: end of batch 23; loss: 0.3289577066898346\n",
      "...Evaluating: start of batch 24\n",
      "...Evaluating: end of batch 24; loss: 0.33220797777175903\n",
      "...Evaluating: start of batch 25\n",
      "...Evaluating: end of batch 25; loss: 0.3358355760574341\n",
      "...Evaluating: start of batch 26\n",
      "...Evaluating: end of batch 26; loss: 0.3319954574108124\n",
      "...Evaluating: start of batch 27\n",
      "...Evaluating: end of batch 27; loss: 0.32664206624031067\n",
      "...Evaluating: start of batch 28\n",
      "...Evaluating: end of batch 28; loss: 0.3233335614204407\n",
      "...Evaluating: start of batch 29\n",
      "...Evaluating: end of batch 29; loss: 0.32625457644462585\n",
      "...Evaluating: start of batch 30\n",
      "...Evaluating: end of batch 30; loss: 0.32139989733695984\n",
      "...Evaluating: start of batch 31\n",
      "...Evaluating: end of batch 31; loss: 0.31940633058547974\n",
      "...Evaluating: start of batch 32\n",
      "...Evaluating: end of batch 32; loss: 0.3228679597377777\n",
      "...Evaluating: start of batch 33\n",
      "...Evaluating: end of batch 33; loss: 0.3193518817424774\n",
      "...Evaluating: start of batch 34\n",
      "...Evaluating: end of batch 34; loss: 0.3170475363731384\n",
      "...Evaluating: start of batch 35\n",
      "...Evaluating: end of batch 35; loss: 0.31655779480934143\n",
      "...Evaluating: start of batch 36\n",
      "...Evaluating: end of batch 36; loss: 0.3167616128921509\n",
      "...Evaluating: start of batch 37\n",
      "...Evaluating: end of batch 37; loss: 0.31282466650009155\n",
      "...Evaluating: start of batch 38\n",
      "...Evaluating: end of batch 38; loss: 0.3148030638694763\n",
      "...Evaluating: start of batch 39\n",
      "...Evaluating: end of batch 39; loss: 0.31390365958213806\n",
      "...Evaluating: start of batch 40\n",
      "...Evaluating: end of batch 40; loss: 0.31594419479370117\n",
      "...Evaluating: start of batch 41\n",
      "...Evaluating: end of batch 41; loss: 0.3127824068069458\n",
      "...Evaluating: start of batch 42\n",
      "...Evaluating: end of batch 42; loss: 0.30972281098365784\n",
      "...Evaluating: start of batch 43\n",
      "...Evaluating: end of batch 43; loss: 0.30687546730041504\n",
      "...Evaluating: start of batch 44\n",
      "...Evaluating: end of batch 44; loss: 0.3050124943256378\n",
      "...Evaluating: start of batch 45\n",
      "...Evaluating: end of batch 45; loss: 0.3066669702529907\n",
      "...Evaluating: start of batch 46\n",
      "...Evaluating: end of batch 46; loss: 0.30776625871658325\n",
      "...Evaluating: start of batch 47\n",
      "...Evaluating: end of batch 47; loss: 0.3043917119503021\n",
      "...Evaluating: start of batch 48\n",
      "...Evaluating: end of batch 48; loss: 0.30601266026496887\n",
      "...Evaluating: start of batch 49\n",
      "...Evaluating: end of batch 49; loss: 0.3066982328891754\n",
      "...Evaluating: start of batch 50\n",
      "...Evaluating: end of batch 50; loss: 0.3047787845134735\n",
      "...Evaluating: start of batch 51\n",
      "...Evaluating: end of batch 51; loss: 0.30214303731918335\n",
      "...Evaluating: start of batch 52\n",
      "...Evaluating: end of batch 52; loss: 0.29919159412384033\n",
      "...Evaluating: start of batch 53\n",
      "...Evaluating: end of batch 53; loss: 0.2999100387096405\n",
      "...Evaluating: start of batch 54\n",
      "...Evaluating: end of batch 54; loss: 0.3022725284099579\n",
      "...Evaluating: start of batch 55\n",
      "...Evaluating: end of batch 55; loss: 0.30168527364730835\n",
      "...Evaluating: start of batch 56\n",
      "...Evaluating: end of batch 56; loss: 0.3056785464286804\n",
      "...Evaluating: start of batch 57\n",
      "...Evaluating: end of batch 57; loss: 0.3072704076766968\n",
      "...Evaluating: start of batch 58\n",
      "...Evaluating: end of batch 58; loss: 0.30535995960235596\n",
      "...Evaluating: start of batch 59\n",
      "...Evaluating: end of batch 59; loss: 0.30519339442253113\n",
      "...Evaluating: start of batch 60\n",
      "...Evaluating: end of batch 60; loss: 0.30655595660209656\n",
      "...Evaluating: start of batch 61\n",
      "...Evaluating: end of batch 61; loss: 0.3070647418498993\n",
      "...Evaluating: start of batch 62\n",
      "...Evaluating: end of batch 62; loss: 0.3064812123775482\n",
      "...Evaluating: start of batch 63\n",
      "...Evaluating: end of batch 63; loss: 0.30564263463020325\n",
      "...Evaluating: start of batch 64\n",
      "...Evaluating: end of batch 64; loss: 0.3046718239784241\n",
      "...Evaluating: start of batch 65\n",
      "...Evaluating: end of batch 65; loss: 0.3040139377117157\n",
      "...Evaluating: start of batch 66\n",
      "...Evaluating: end of batch 66; loss: 0.30834850668907166\n",
      "...Evaluating: start of batch 67\n",
      "...Evaluating: end of batch 67; loss: 0.3056007921695709\n",
      "...Evaluating: start of batch 68\n",
      "...Evaluating: end of batch 68; loss: 0.3051419258117676\n",
      "...Evaluating: start of batch 69\n",
      "...Evaluating: end of batch 69; loss: 0.3047947585582733\n",
      "...Evaluating: start of batch 70\n",
      "...Evaluating: end of batch 70; loss: 0.3074972629547119\n",
      "...Evaluating: start of batch 71\n",
      "...Evaluating: end of batch 71; loss: 0.31048738956451416\n",
      "...Evaluating: start of batch 72\n",
      "...Evaluating: end of batch 72; loss: 0.3089413344860077\n",
      "...Evaluating: start of batch 73\n",
      "...Evaluating: end of batch 73; loss: 0.31120333075523376\n",
      "...Evaluating: start of batch 74\n",
      "...Evaluating: end of batch 74; loss: 0.30915430188179016\n",
      "...Evaluating: start of batch 75\n",
      "...Evaluating: end of batch 75; loss: 0.3136179745197296\n",
      "...Evaluating: start of batch 76\n",
      "...Evaluating: end of batch 76; loss: 0.3141895532608032\n",
      "...Evaluating: start of batch 77\n",
      "...Evaluating: end of batch 77; loss: 0.3133188486099243\n",
      "...Evaluating: start of batch 78\n",
      "...Evaluating: end of batch 78; loss: 0.3133831024169922\n",
      "...Evaluating: start of batch 79\n",
      "...Evaluating: end of batch 79; loss: 0.31310150027275085\n",
      "...Evaluating: start of batch 80\n",
      "...Evaluating: end of batch 80; loss: 0.31195542216300964\n",
      "...Evaluating: start of batch 81\n",
      "...Evaluating: end of batch 81; loss: 0.30974477529525757\n",
      "...Evaluating: start of batch 82\n",
      "...Evaluating: end of batch 82; loss: 0.31046733260154724\n",
      "...Evaluating: start of batch 83\n",
      "...Evaluating: end of batch 83; loss: 0.31155917048454285\n",
      "...Evaluating: start of batch 84\n",
      "...Evaluating: end of batch 84; loss: 0.3099343776702881\n",
      "...Evaluating: start of batch 85\n",
      "...Evaluating: end of batch 85; loss: 0.31946471333503723\n",
      "...Evaluating: start of batch 86\n",
      "...Evaluating: end of batch 86; loss: 0.31888896226882935\n",
      "...Evaluating: start of batch 87\n",
      "...Evaluating: end of batch 87; loss: 0.3183823227882385\n",
      "...Evaluating: start of batch 88\n",
      "...Evaluating: end of batch 88; loss: 0.3194204866886139\n",
      "...Evaluating: start of batch 89\n",
      "...Evaluating: end of batch 89; loss: 0.3194873631000519\n",
      "...Evaluating: start of batch 90\n",
      "...Evaluating: end of batch 90; loss: 0.317139208316803\n",
      "...Evaluating: start of batch 91\n",
      "...Evaluating: end of batch 91; loss: 0.3154810667037964\n",
      "...Evaluating: start of batch 92\n",
      "...Evaluating: end of batch 92; loss: 0.315006285905838\n",
      "...Evaluating: start of batch 93\n",
      "...Evaluating: end of batch 93; loss: 0.3128091096878052\n",
      "...Evaluating: start of batch 94\n",
      "...Evaluating: end of batch 94; loss: 0.31045982241630554\n",
      "...Evaluating: start of batch 95\n",
      "...Evaluating: end of batch 95; loss: 0.3099479377269745\n",
      "...Evaluating: start of batch 96\n",
      "...Evaluating: end of batch 96; loss: 0.30783066153526306\n",
      "...Evaluating: start of batch 97\n",
      "...Evaluating: end of batch 97; loss: 0.32456618547439575\n",
      "...Evaluating: start of batch 98\n",
      "...Evaluating: end of batch 98; loss: 0.3253576159477234\n",
      "...Evaluating: start of batch 99\n",
      "...Evaluating: end of batch 99; loss: 0.3244161605834961\n",
      "...Evaluating: start of batch 100\n",
      "...Evaluating: end of batch 100; loss: 0.3244495689868927\n",
      "...Evaluating: start of batch 101\n",
      "...Evaluating: end of batch 101; loss: 0.32384151220321655\n",
      "...Evaluating: start of batch 102\n",
      "...Evaluating: end of batch 102; loss: 0.3219357430934906\n",
      "...Evaluating: start of batch 103\n",
      "...Evaluating: end of batch 103; loss: 0.32178381085395813\n",
      "...Evaluating: start of batch 104\n",
      "...Evaluating: end of batch 104; loss: 0.3217299282550812\n",
      "...Evaluating: start of batch 105\n",
      "...Evaluating: end of batch 105; loss: 0.3210504353046417\n",
      "...Evaluating: start of batch 106\n",
      "...Evaluating: end of batch 106; loss: 0.3207271695137024\n",
      "...Evaluating: start of batch 107\n",
      "...Evaluating: end of batch 107; loss: 0.3205525577068329\n",
      "...Evaluating: start of batch 108\n",
      "...Evaluating: end of batch 108; loss: 0.3210132420063019\n",
      "...Evaluating: start of batch 109\n",
      "...Evaluating: end of batch 109; loss: 0.3198583722114563\n",
      "...Evaluating: start of batch 110\n",
      "...Evaluating: end of batch 110; loss: 0.3195211589336395\n",
      "...Evaluating: start of batch 111\n",
      "...Evaluating: end of batch 111; loss: 0.31868597865104675\n",
      "...Evaluating: start of batch 112\n",
      "...Evaluating: end of batch 112; loss: 0.3199648857116699\n",
      "...Evaluating: start of batch 113\n",
      "...Evaluating: end of batch 113; loss: 0.3183853030204773\n",
      "...Evaluating: start of batch 114\n",
      "...Evaluating: end of batch 114; loss: 0.3198777139186859\n",
      "...Evaluating: start of batch 115\n",
      "...Evaluating: end of batch 115; loss: 0.3206404745578766\n",
      "...Evaluating: start of batch 116\n",
      "...Evaluating: end of batch 116; loss: 0.32407838106155396\n",
      "...Evaluating: start of batch 117\n",
      "...Evaluating: end of batch 117; loss: 0.3247534930706024\n",
      "...Evaluating: start of batch 118\n",
      "...Evaluating: end of batch 118; loss: 0.3230065405368805\n",
      "...Evaluating: start of batch 119\n",
      "...Evaluating: end of batch 119; loss: 0.3280847668647766\n",
      "...Evaluating: start of batch 120\n",
      "...Evaluating: end of batch 120; loss: 0.3270256221294403\n",
      "Stop testing\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3270 - val_loss: 0.3270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 0 of training; loss: 0.3269706964492798\n",
      "Stop training\n",
      "Start predicting\n",
      "...Predicting: start of batch 0\n",
      "...Predicting: end of batch 0; output: [[0.6734809]\n",
      " [1.7383174]\n",
      " [4.2687154]]\n",
      "Stop predicting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6734809],\n",
       "       [1.7383174],\n",
       "       [4.2687154]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Stop training\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(\"Start epoch {} of training\".format(epoch))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"End epoch {} of training; loss: {}\".format(epoch, logs['loss']))\n",
    "    \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        print(\"...Training: start of batch {}\".format(batch))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"...Training: end of batch {}; loss: {}\".format(batch, logs['loss']))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        print(\"Start testing\")\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        print(\"Stop testing\")\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        print(\"...Evaluating: start of batch {}\".format(batch))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\"...Evaluating: end of batch {}; loss: {}\".format(batch, logs['loss']))\n",
    "        \n",
    "    def on_predict_begin(self, logs=None):\n",
    "        print(\"Start predicting\")\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting\")\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}\".format(batch))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; output: {}\".format(batch, logs['outputs']))\n",
    "        \n",
    "custom_cb = CustomCallback()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[custom_cb])\n",
    "model.predict(X_new, callbacks=[custom_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard可视化\n",
    "\n",
    "用途：\n",
    "1. 在训练期间查看学习曲线\n",
    "2. 比较多次运行的学习曲线\n",
    "3. 可视化计算图\n",
    "4. 分析训练统计数据\n",
    "5. 查看由模型生成的图像\n",
    "6. 高维数据投影到3D空间\n",
    "7. 聚类并可视化\n",
    "\n",
    "\n",
    "使用方式分为三步：\n",
    "\n",
    "1. 定义tensorboard的日志目录以及可以生成当前日期和时间的子目录函数\n",
    "2. 定义一个`TensorBoard()`回调\n",
    "3. 开始训练，然后启动tensorboard服务\n",
    "\n",
    "每一次运行model.fit,都会产生一个目录，目录结构如下所示（显示了两次运行结果）：\n",
    "\n",
    "- 一个是用于训练日志的子目录\n",
    "- 一个是用于验证日志的子目录\n",
    "\n",
    "![image-20210315075556996](images/image-20210315075556996.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:19:27.379568Z",
     "start_time": "2021-03-15T00:19:26.572062Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_new = X_test[:3]\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", \n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义日志目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:19:31.483682Z",
     "start_time": "2021-03-15T00:19:31.467725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory .\\my_logs failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2021_03_15-08_19_31'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_run_logdir(logpath=\"my_logs\"):\n",
    "    root_logdir = os.path.join(os.curdir, logpath)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(root_logdir)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % root_logdir)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % root_logdir)\n",
    "\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义TensorBoard回调\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:20:48.995055Z",
     "start_time": "2021-03-15T00:19:41.422216Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 3.3697 - val_loss: 0.7125\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6964 - val_loss: 0.6880\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6167 - val_loss: 0.5802\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5845 - val_loss: 0.5166\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5321 - val_loss: 0.4895\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5083 - val_loss: 0.4947\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5044 - val_loss: 0.4860\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4813 - val_loss: 0.4550\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4628 - val_loss: 0.4412\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4549 - val_loss: 0.4377\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4416 - val_loss: 0.4400\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4295 - val_loss: 0.4515\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4326 - val_loss: 0.3998\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4207 - val_loss: 0.3957\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4199 - val_loss: 0.3915\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4248 - val_loss: 0.3935\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4105 - val_loss: 0.3809\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4069 - val_loss: 0.3794\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3902 - val_loss: 0.3850\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3864 - val_loss: 0.3810\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3978 - val_loss: 0.3702\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3816 - val_loss: 0.3781\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4042 - val_loss: 0.3649\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3823 - val_loss: 0.3655\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3792 - val_loss: 0.3611\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3800 - val_loss: 0.3624\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3858 - val_loss: 0.3564\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3839 - val_loss: 0.3580\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3736 - val_loss: 0.3560\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3843 - val_loss: 0.3547\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3751 - val_loss: 0.3502\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3668 - val_loss: 0.3643\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3737 - val_loss: 0.3468\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3613 - val_loss: 0.3474\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3580 - val_loss: 0.3626\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3562 - val_loss: 0.3438\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3889 - val_loss: 0.3612\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3550 - val_loss: 0.3460\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3789 - val_loss: 0.3491\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3580 - val_loss: 0.3433\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3771 - val_loss: 0.3556\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3553 - val_loss: 0.3486\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3619 - val_loss: 0.3689\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3485 - val_loss: 0.3601\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3636 - val_loss: 0.3401\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3569 - val_loss: 0.3508\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3694 - val_loss: 0.3485\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3468 - val_loss: 0.3659\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3589 - val_loss: 0.3358\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3618 - val_loss: 0.3324\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3548 - val_loss: 0.3466\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3463 - val_loss: 0.3310\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3526 - val_loss: 0.3834\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3391 - val_loss: 0.3302\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3520 - val_loss: 0.3534\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3396 - val_loss: 0.3289\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3452 - val_loss: 0.3281\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3367 - val_loss: 0.3719\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3448 - val_loss: 0.3351\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3427 - val_loss: 0.3512\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3571 - val_loss: 0.3529\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3525 - val_loss: 0.3448\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3470 - val_loss: 0.3341\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3257 - val_loss: 0.3252\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3422 - val_loss: 0.3375\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3368 - val_loss: 0.3290\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3517 - val_loss: 0.3586\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3361 - val_loss: 0.3318\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3404 - val_loss: 0.3366\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3360 - val_loss: 0.3570\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3413 - val_loss: 0.3604\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3243 - val_loss: 0.3322\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3414 - val_loss: 0.3240\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3364 - val_loss: 0.3830\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3307 - val_loss: 0.3283\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3363 - val_loss: 0.3463\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3260 - val_loss: 0.3479\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3367 - val_loss: 0.3476\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3348 - val_loss: 0.3402\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3364 - val_loss: 0.3386\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3336 - val_loss: 0.3329\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3478 - val_loss: 0.3195\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3407 - val_loss: 0.3306\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3217 - val_loss: 0.3710\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3270 - val_loss: 0.3215\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3222 - val_loss: 0.3184\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3432 - val_loss: 0.3573\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3299 - val_loss: 0.3317\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3244 - val_loss: 0.3643\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3226 - val_loss: 0.3458\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3266 - val_loss: 0.3253\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3317 - val_loss: 0.3474\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3460 - val_loss: 0.3198\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3243 - val_loss: 0.3424\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3263 - val_loss: 0.3422\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3375 - val_loss: 0.3264\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('models/my_keras_model.h5', save_best_only=True)\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, earlystop_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启动TensorBoard服务\n",
    "\n",
    "\n",
    "\n",
    "### 终端启动\n",
    "\n",
    "\n",
    "```bash\n",
    "$ tensorboard --logdir=./my_logs --port=6006\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jupyter 中启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:05:46.097114Z",
     "start_time": "2021-03-15T00:04:45.723332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 16912."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较两次运行过程\n",
    "\n",
    "例如模拟如下场景，通过调整学习率后再次训练模型，通过如下的tensorboard可以看到两次的收敛过程，很明显第二次的学习率更好一点\n",
    "\n",
    "![image-20210315081519157](images/image-20210315081519157.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:21:08.219489Z",
     "start_time": "2021-03-15T00:21:08.175605Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", \n",
    "              optimizer=keras.optimizers.SGD(lr=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:21:33.508949Z",
     "start_time": "2021-03-15T00:21:14.765509Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory .\\my_logs failed\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.8112 - val_loss: 3.4096\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7308 - val_loss: 1.7776\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4232 - val_loss: 0.4049\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3825 - val_loss: 0.3987\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3775 - val_loss: 0.3372\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3495 - val_loss: 0.3327\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3547 - val_loss: 0.3173\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3430 - val_loss: 0.3041\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3288 - val_loss: 0.2993\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3288 - val_loss: 0.3117\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3166 - val_loss: 0.3005\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3141 - val_loss: 0.3023\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3100 - val_loss: 0.2856\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3043 - val_loss: 0.2823\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3006 - val_loss: 0.2833\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3091 - val_loss: 0.2773\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2877 - val_loss: 0.3314\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2937 - val_loss: 0.2812\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2915 - val_loss: 0.2809\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2859 - val_loss: 0.3114\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2862 - val_loss: 0.2934\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2760 - val_loss: 0.2893\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2948 - val_loss: 0.3241\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2814 - val_loss: 0.4248\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2829 - val_loss: 2.1442\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2923 - val_loss: 0.2786\n"
     ]
    }
   ],
   "source": [
    "run_logdir2 = get_run_logdir()\n",
    "run_logdir2\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir2)\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, earlystop_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看Tensorboard的其他设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(keras.callbacks.TensorBoard.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数调整\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_new = X_test[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隐藏层数\n",
    "\n",
    "\n",
    "- 简单问题：单层MLP\n",
    "- 复杂问题：深层MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单元数\n",
    "\n",
    "通常做法：选择一个比你实际需要的层和神经元更多的模型，然后使用提前停止和其他正则化技术防止模型过拟合\n",
    "\n",
    "注意：通常增加层数而不是每层的神经元数，将获得更多收益\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "\n",
    "大批量：更利于GPU加速，在GPU显存允许的情况下，但是大批量会导致训练不稳定\n",
    "\n",
    "通常做法：尝试使用大批量，慢慢增加学习率，如果训练不稳定或最终表现不佳，可以使用小批量\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合sklearn进行随机搜索\n",
    "\n",
    "模型参数和超参数到底选择哪些，通常最简单的思路就是尝试各种参数的组合，然后查看哪种对验证集最有效，可以使用GridSearchCV或RandomizedSearchCV,但是要使用scikit-learn中的这两个功能，需要将keras模型包装在常规scikit-learn对象中。\n",
    "\n",
    "那么为了包装这个模型需要进行两步：\n",
    "- 构建一个函数，函数中包括两部分，模型构建以及模型编译 \n",
    "- 基于该函数, 如果是回归模型，使用`keras.wrappers.scikit_learn.KerasRegressor`,如果是分类模型，使用`keras.wrappers.scikit-learn.KerasClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:41:59.410811Z",
     "start_time": "2021-03-15T00:41:59.379446Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# step 1: build model function\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# step 2: wrap model function\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "# wrap后的对象，可以像一般的scikit-learn模型一样训练，评估，预测，如下方\n",
    "# keras_reg.fit(X_train, y_train, \n",
    "#               epochs=100,\n",
    "#               validation_data=(X_valid, y_valid),\n",
    "#               callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "# mse_test = keras_reg.score(X_test, y_test)\n",
    "\n",
    "# y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:52:59.657708Z",
     "start_time": "2021-03-15T00:42:03.888810Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.1547 - val_loss: 1.7081\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4469 - val_loss: 0.9110\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8676 - val_loss: 0.7824\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7414 - val_loss: 0.7247\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6814 - val_loss: 0.6972\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.9293\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.8525\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6228 - val_loss: 0.8565\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6164 - val_loss: 0.6357\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6057 - val_loss: 0.8564\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 0.8620\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5541 - val_loss: 0.5836\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5937 - val_loss: 0.9029\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5841 - val_loss: 0.9560\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 0.6270\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5553 - val_loss: 0.7788\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5846 - val_loss: 0.7895\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5781 - val_loss: 0.8733\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5554 - val_loss: 0.8946\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5624 - val_loss: 0.8635\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5384 - val_loss: 0.5858\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5338 - val_loss: 0.7993\n",
      "121/121 [==============================] - 0s 731us/step - loss: 0.5426\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=   9.3s\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.7957 - val_loss: 26.9061\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.6384 - val_loss: 26.8497\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8660 - val_loss: 26.7977\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6878 - val_loss: 26.4974\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6382 - val_loss: 26.1660\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 25.8528\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5995 - val_loss: 25.4913\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5955 - val_loss: 25.2193\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5781 - val_loss: 24.9058\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5596 - val_loss: 24.4286\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5661 - val_loss: 24.2278\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5490 - val_loss: 24.0258\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5553 - val_loss: 23.8711\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5104 - val_loss: 23.4690\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5673 - val_loss: 23.2472\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5378 - val_loss: 23.0739\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 22.7865\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5382 - val_loss: 22.5360\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5349 - val_loss: 22.4090\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5083 - val_loss: 22.1278\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5397 - val_loss: 22.1408\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5090 - val_loss: 21.9335\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 21.8539\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4929 - val_loss: 21.6222\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4914 - val_loss: 21.5186\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5041 - val_loss: 21.3988\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5341 - val_loss: 21.3768\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5134 - val_loss: 21.2476\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5176 - val_loss: 21.2832\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 21.3075\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5110 - val_loss: 21.2469\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5077 - val_loss: 21.1862\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5101 - val_loss: 21.1453\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4906 - val_loss: 21.0353\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4952 - val_loss: 21.0231\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4895 - val_loss: 20.9072\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5080 - val_loss: 20.8950\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4890 - val_loss: 20.8829\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5006 - val_loss: 20.7558\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4946 - val_loss: 20.8037\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4658 - val_loss: 20.7423\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4918 - val_loss: 20.6682\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5127 - val_loss: 20.6167\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5149 - val_loss: 20.6802\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4780 - val_loss: 20.6033\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5299 - val_loss: 20.6674\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4895 - val_loss: 20.5505\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4714 - val_loss: 20.4242\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 20.5209\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4902 - val_loss: 20.5152\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5013 - val_loss: 20.4692\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 20.5355\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4856 - val_loss: 20.4156\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5012 - val_loss: 20.4309\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4855 - val_loss: 20.5274\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5175 - val_loss: 20.5075\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 20.5514\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 20.5769\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5145 - val_loss: 20.4915\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4942 - val_loss: 20.3295\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5137 - val_loss: 20.3797\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5056 - val_loss: 20.3691\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5144 - val_loss: 20.3979\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5105 - val_loss: 20.3882\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4998 - val_loss: 20.4981\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 20.5639\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4925 - val_loss: 20.5205\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4835 - val_loss: 20.4924\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5083 - val_loss: 20.5108\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 20.5535\n",
      "121/121 [==============================] - 0s 798us/step - loss: 0.9941\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=  28.9s\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.4641 - val_loss: 1.7267\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4595 - val_loss: 0.9431\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7767 - val_loss: 0.7042\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6471 - val_loss: 0.9673\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5993 - val_loss: 0.5837\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.6619\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.6855\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 0.9454\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5858 - val_loss: 0.6909\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5525 - val_loss: 0.5410\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5736 - val_loss: 0.8713\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5524 - val_loss: 0.5866\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5198 - val_loss: 0.5302\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.6302\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5612 - val_loss: 0.5418\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5552 - val_loss: 0.7731\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5366 - val_loss: 0.5713\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5580 - val_loss: 0.5495\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5533 - val_loss: 0.8516\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 0.6171\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.6037\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 0.6406\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5375 - val_loss: 0.5159\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 0.5618\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 0.5417\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 0.4991\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5412 - val_loss: 0.8453\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5345 - val_loss: 0.5855\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 0.7574\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.6030\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5145 - val_loss: 0.6864\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5551 - val_loss: 0.7683\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5437 - val_loss: 0.7585\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 0.8117\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5227 - val_loss: 0.8037\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5190\n",
      "121/121 [==============================] - 0s 781us/step - loss: 0.5368\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=  14.6s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.3407 - val_loss: 2.2065\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 50.8455\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7660 - val_loss: 106.2636\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9442 - val_loss: 650.6271\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.4056 - val_loss: 2052.5581\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 53.9453 - val_loss: 7937.4839\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 75.3782 - val_loss: 30964.6855\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 565.7549 - val_loss: 122503.8750\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 581.0503 - val_loss: 478198.4688\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 10659.7328 - val_loss: 1988065.2500\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 29334.0124 - val_loss: 7792845.0000\n",
      "121/121 [==============================] - 0s 756us/step - loss: 20650.4941\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   4.8s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.7389 - val_loss: 6.7873\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6371 - val_loss: 15.9220\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5770 - val_loss: 21.7265\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5339 - val_loss: 22.7163\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5196 - val_loss: 22.7142\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5103 - val_loss: 22.4581\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4932 - val_loss: 21.6235\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5065 - val_loss: 22.0138\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4993 - val_loss: 21.0501\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4811 - val_loss: 17.7046\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5064 - val_loss: 19.7294\n",
      "121/121 [==============================] - 0s 773us/step - loss: 0.9672\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   4.8s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.9011 - val_loss: 245.2250\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4718 - val_loss: 352.7937\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9047 - val_loss: 1056.4773\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 32.6172 - val_loss: 2128.5037\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.1646 - val_loss: 5346.5103\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 23.4769 - val_loss: 9527.1572\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 36.5427 - val_loss: 18975.7773\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 614.3579 - val_loss: 37178.6719\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 217.5102 - val_loss: 72244.0156\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 98.1784 - val_loss: 135352.1406\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5586.4306 - val_loss: 260482.3281\n",
      "121/121 [==============================] - 0s 766us/step - loss: 285.8345\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   4.8s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.8106 - val_loss: 2.7669\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2058 - val_loss: 1.0638\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8905 - val_loss: 0.8127\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7921 - val_loss: 0.7267\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7390 - val_loss: 0.6844\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6861 - val_loss: 0.6565\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6801 - val_loss: 0.6332\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6441 - val_loss: 0.6141\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6172 - val_loss: 0.6055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6170 - val_loss: 0.5774\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5610\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5481 - val_loss: 0.5509\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5737 - val_loss: 0.5313\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5567 - val_loss: 0.5243\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5461 - val_loss: 0.5141\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5128 - val_loss: 0.4989\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 0.4958\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5139 - val_loss: 0.4792\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4949 - val_loss: 0.4749\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4924 - val_loss: 0.4628\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4753 - val_loss: 0.4732\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4567 - val_loss: 0.4562\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4727 - val_loss: 0.4490\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4525 - val_loss: 0.4359\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4562 - val_loss: 0.4444\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4353 - val_loss: 0.4445\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4546 - val_loss: 0.4318\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4456 - val_loss: 0.4502\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4435 - val_loss: 0.4345\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4273 - val_loss: 0.4285\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4133 - val_loss: 0.4187\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4131 - val_loss: 0.4237\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4087 - val_loss: 0.4287\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4087 - val_loss: 0.4305\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4024 - val_loss: 0.4173\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3997 - val_loss: 0.4229\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4061 - val_loss: 0.4160\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3956 - val_loss: 0.4264\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4007 - val_loss: 0.4093\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3797 - val_loss: 0.4280\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 0.4174\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4009 - val_loss: 0.4122\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.4078\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3865 - val_loss: 0.4327\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3904 - val_loss: 0.4127\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3778 - val_loss: 0.4068\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3852 - val_loss: 0.4219\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3673 - val_loss: 0.4083\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3900 - val_loss: 0.4076\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 0.4207\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3652 - val_loss: 0.4074\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3919 - val_loss: 0.4080\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3759 - val_loss: 0.4097\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3794 - val_loss: 0.3915\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3610 - val_loss: 0.4151\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.4040\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3721 - val_loss: 0.4093\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3786 - val_loss: 0.4229\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3782 - val_loss: 0.3957\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3647 - val_loss: 0.3970\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3577 - val_loss: 0.4052\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3603 - val_loss: 0.4197\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3753 - val_loss: 0.3970\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3603 - val_loss: 0.4202\n",
      "121/121 [==============================] - 0s 943us/step - loss: 0.3856\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  32.1s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.3831 - val_loss: 5.3086\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0288 - val_loss: 5.3787\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8161 - val_loss: 3.9616\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7723 - val_loss: 2.8512\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7321 - val_loss: 2.0525\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7008 - val_loss: 1.4995\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6760 - val_loss: 1.0861\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6388 - val_loss: 0.8078\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6385 - val_loss: 0.6582\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6136 - val_loss: 0.5863\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5967 - val_loss: 0.5734\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5677 - val_loss: 0.6204\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5628 - val_loss: 0.6940\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5466 - val_loss: 0.7567\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5704 - val_loss: 0.8715\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5418 - val_loss: 0.9568\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5339 - val_loss: 1.0548\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5280 - val_loss: 1.1415\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5097 - val_loss: 1.2358\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4972 - val_loss: 1.2624\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4939 - val_loss: 1.3328\n",
      "121/121 [==============================] - 0s 906us/step - loss: 0.5215\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  11.2s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 3.9418 - val_loss: 3.2726\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2721 - val_loss: 1.4689\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8112 - val_loss: 0.7975\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7339 - val_loss: 0.6734\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6713 - val_loss: 0.6376\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6347 - val_loss: 0.6141\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6246 - val_loss: 0.5924\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6098 - val_loss: 0.5856\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5971 - val_loss: 0.5549\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5568 - val_loss: 0.5363\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5572 - val_loss: 0.5225\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5343 - val_loss: 0.5080\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5016 - val_loss: 0.4960\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5066 - val_loss: 0.4860\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5216 - val_loss: 0.4762\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5039 - val_loss: 0.4645\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4758 - val_loss: 0.4564\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4927 - val_loss: 0.4479\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4781 - val_loss: 0.4404\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4615 - val_loss: 0.4351\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4492 - val_loss: 0.4306\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4725 - val_loss: 0.4245\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4422 - val_loss: 0.4221\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4316 - val_loss: 0.4240\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4329 - val_loss: 0.4125\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4290 - val_loss: 0.4098\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4363 - val_loss: 0.4059\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4325 - val_loss: 0.4073\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4151 - val_loss: 0.4134\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.3991\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.3942\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4297 - val_loss: 0.4034\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4117 - val_loss: 0.3938\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3956 - val_loss: 0.4066\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.3906\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.3929\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4014 - val_loss: 0.3883\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3965 - val_loss: 0.3921\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4096 - val_loss: 0.3849\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3959 - val_loss: 0.3837\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3724 - val_loss: 0.4017\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3786 - val_loss: 0.3974\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3915 - val_loss: 0.3765\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3965 - val_loss: 0.3867\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3802 - val_loss: 0.3841\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3773 - val_loss: 0.3806\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.3726\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3799 - val_loss: 0.3740\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3849 - val_loss: 0.3916\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.3908\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3908 - val_loss: 0.3863\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3728 - val_loss: 0.3901\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3756 - val_loss: 0.3789\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3730 - val_loss: 0.3703\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3732 - val_loss: 0.3732\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3759 - val_loss: 0.3846\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3680 - val_loss: 0.3949\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3802 - val_loss: 0.3856\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3739 - val_loss: 0.3862\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3856 - val_loss: 0.3703\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3836 - val_loss: 0.3782\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3749 - val_loss: 0.3881\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3655 - val_loss: 0.3728\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3810 - val_loss: 0.3697\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3659\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3614 - val_loss: 0.3801\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3726 - val_loss: 0.3628\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.3802\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3563 - val_loss: 0.3672\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3632 - val_loss: 0.3734\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3830 - val_loss: 0.3659\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3486 - val_loss: 0.3680\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3606\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3634 - val_loss: 0.3617\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3535 - val_loss: 0.3746\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3665 - val_loss: 0.3834\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3656 - val_loss: 0.3682\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3613 - val_loss: 0.3594\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3640 - val_loss: 0.3584\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3565 - val_loss: 0.3707\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3489 - val_loss: 0.3620\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3559 - val_loss: 0.3693\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3461 - val_loss: 0.3642\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3643 - val_loss: 0.3748\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3550 - val_loss: 0.3525\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3471 - val_loss: 0.3574\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3549 - val_loss: 0.3746\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3395 - val_loss: 0.3638\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3616 - val_loss: 0.3724\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3495 - val_loss: 0.3577\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3596 - val_loss: 0.3584\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.3518\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3408 - val_loss: 0.3656\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3620 - val_loss: 0.3688\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3633 - val_loss: 0.3665\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3474 - val_loss: 0.3683\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3482 - val_loss: 0.3687\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3402 - val_loss: 0.3515\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3379 - val_loss: 0.3660\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3365 - val_loss: 0.3757\n",
      "121/121 [==============================] - 0s 889us/step - loss: 0.3508\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  50.4s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 4.3929 - val_loss: 2.9733\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.0446 - val_loss: 2.5510\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2961 - val_loss: 1.9182\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0282 - val_loss: 1.3906\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8298 - val_loss: 1.0474\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7180 - val_loss: 0.8336\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6888 - val_loss: 0.7061\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6286 - val_loss: 0.6297\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5941 - val_loss: 0.5843\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5838 - val_loss: 0.5538\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5513 - val_loss: 0.5334\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5204 - val_loss: 0.5187\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5468 - val_loss: 0.5065\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5371 - val_loss: 0.4968\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5239 - val_loss: 0.4878\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4975 - val_loss: 0.4802\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5189 - val_loss: 0.4735\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4971 - val_loss: 0.4673\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4819 - val_loss: 0.4614\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4809 - val_loss: 0.4559\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4666 - val_loss: 0.4508\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4495 - val_loss: 0.4462\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4700 - val_loss: 0.4417\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4482 - val_loss: 0.4383\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4528 - val_loss: 0.4340\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4331 - val_loss: 0.4303\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4566 - val_loss: 0.4271\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4476 - val_loss: 0.4239\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4520 - val_loss: 0.4209\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4380 - val_loss: 0.4181\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4262 - val_loss: 0.4155\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4269 - val_loss: 0.4127\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4210 - val_loss: 0.4101\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4207 - val_loss: 0.4077\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4188 - val_loss: 0.4054\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4147 - val_loss: 0.4034\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4233 - val_loss: 0.4008\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4105 - val_loss: 0.3986\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4160 - val_loss: 0.3963\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3957 - val_loss: 0.3945\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4118 - val_loss: 0.3927\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4227 - val_loss: 0.3912\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4110 - val_loss: 0.3898\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4090 - val_loss: 0.3890\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4141 - val_loss: 0.3881\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3956 - val_loss: 0.3877\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4073 - val_loss: 0.3880\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3900 - val_loss: 0.3881\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4142 - val_loss: 0.3893\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4004 - val_loss: 0.3898\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3863 - val_loss: 0.3900\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4125 - val_loss: 0.3918\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3999 - val_loss: 0.3932\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4010 - val_loss: 0.3941\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3834 - val_loss: 0.3952\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4274 - val_loss: 0.3969\n",
      "121/121 [==============================] - 0s 956us/step - loss: 0.4236\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  31.4s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 3ms/step - loss: 4.1377 - val_loss: 4.5514\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4427 - val_loss: 5.0236\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8559 - val_loss: 4.1466\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7672 - val_loss: 3.2850\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6805 - val_loss: 2.7414\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6510 - val_loss: 2.4268\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6253 - val_loss: 2.1792\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5960 - val_loss: 1.9922\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5839 - val_loss: 1.8505\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5788 - val_loss: 1.7316\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5682 - val_loss: 1.6235\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5340 - val_loss: 1.4790\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5305 - val_loss: 1.3828\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5194 - val_loss: 1.3105\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5502 - val_loss: 1.2595\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5151 - val_loss: 1.2444\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5208 - val_loss: 1.2166\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5118 - val_loss: 1.2001\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4958 - val_loss: 1.1805\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4916 - val_loss: 1.1624\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4861 - val_loss: 1.1455\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4907 - val_loss: 1.1397\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4897 - val_loss: 1.1263\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4664 - val_loss: 1.1129\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4496 - val_loss: 1.1007\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4537 - val_loss: 1.0909\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4700 - val_loss: 1.0823\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4671 - val_loss: 1.0699\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4558 - val_loss: 1.0784\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4529 - val_loss: 1.0590\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4411 - val_loss: 1.0380\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4499 - val_loss: 1.0366\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4437 - val_loss: 1.0247\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4351 - val_loss: 1.0248\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4278 - val_loss: 1.0223\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4209 - val_loss: 1.0034\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4367 - val_loss: 1.0094\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4140 - val_loss: 0.9974\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4270 - val_loss: 0.9967\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4074 - val_loss: 0.9929\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4030 - val_loss: 0.9887\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4045 - val_loss: 0.9886\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4275 - val_loss: 0.9759\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4230 - val_loss: 0.9802\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4027 - val_loss: 0.9772\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4006 - val_loss: 0.9652\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4099 - val_loss: 0.9666\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3910 - val_loss: 0.9629\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4188 - val_loss: 0.9656\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3986 - val_loss: 0.9619\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4021 - val_loss: 0.9438\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4115 - val_loss: 0.9477\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3991 - val_loss: 0.9505\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4010 - val_loss: 0.9428\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3937 - val_loss: 0.9439\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4127 - val_loss: 0.9465\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4160 - val_loss: 0.9397\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4147 - val_loss: 0.9408\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4230 - val_loss: 0.9434\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4028 - val_loss: 0.9333\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4040 - val_loss: 0.9383\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4022 - val_loss: 0.9385\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4004 - val_loss: 0.9352\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3958 - val_loss: 0.9361\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3867 - val_loss: 0.9393\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4084 - val_loss: 0.9407\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3876 - val_loss: 0.9311\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3892 - val_loss: 0.9334\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.9342\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4107 - val_loss: 0.9270\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4114 - val_loss: 0.9241\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3927 - val_loss: 0.9286\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4028 - val_loss: 0.9310\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3906 - val_loss: 0.9357\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3925 - val_loss: 0.9352\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3920 - val_loss: 0.9400\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3870 - val_loss: 0.9320\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3762 - val_loss: 0.9341\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3951 - val_loss: 0.9391\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3879 - val_loss: 0.9440\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3681 - val_loss: 0.9490\n",
      "121/121 [==============================] - 0s 989us/step - loss: 0.3996\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  45.2s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 4.6283 - val_loss: 3.1462\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.0885 - val_loss: 3.1081\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2154 - val_loss: 2.3972\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0768 - val_loss: 1.8350\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9147 - val_loss: 1.4074\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8503 - val_loss: 1.1605\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7950 - val_loss: 1.0036\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7897 - val_loss: 0.8745\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7544 - val_loss: 0.8132\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7086 - val_loss: 0.7669\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7197 - val_loss: 0.7445\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6858 - val_loss: 0.7242\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6493 - val_loss: 0.7105\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6462 - val_loss: 0.6983\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6680 - val_loss: 0.6858\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6423 - val_loss: 0.6737\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6117 - val_loss: 0.6624\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6249 - val_loss: 0.6505\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6084 - val_loss: 0.6427\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5883 - val_loss: 0.6345\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5757 - val_loss: 0.6264\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6015 - val_loss: 0.6169\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5583 - val_loss: 0.6075\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5463 - val_loss: 0.6020\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5470 - val_loss: 0.5941\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5196 - val_loss: 0.5848\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5437 - val_loss: 0.5788\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5282 - val_loss: 0.5723\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5162 - val_loss: 0.5692\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5096 - val_loss: 0.5604\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4880 - val_loss: 0.5548\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5094 - val_loss: 0.5537\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4915 - val_loss: 0.5473\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4597 - val_loss: 0.5471\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4757 - val_loss: 0.5434\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4665 - val_loss: 0.5394\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4698 - val_loss: 0.5365\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4618 - val_loss: 0.5329\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4678 - val_loss: 0.5305\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4539 - val_loss: 0.5278\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4288 - val_loss: 0.5289\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4314 - val_loss: 0.5259\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4462 - val_loss: 0.5212\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4529 - val_loss: 0.5214\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4369 - val_loss: 0.5193\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4245 - val_loss: 0.5152\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4358 - val_loss: 0.5128\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4235 - val_loss: 0.5115\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4343 - val_loss: 0.5099\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4166 - val_loss: 0.5089\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4344 - val_loss: 0.5046\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4152 - val_loss: 0.5046\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4162 - val_loss: 0.5034\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4145 - val_loss: 0.5014\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4138 - val_loss: 0.5031\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4134 - val_loss: 0.5013\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4022 - val_loss: 0.4985\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4136 - val_loss: 0.4979\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4134 - val_loss: 0.4955\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4201 - val_loss: 0.4932\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4199 - val_loss: 0.4947\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4107 - val_loss: 0.4925\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.4897\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4145 - val_loss: 0.4878\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4115 - val_loss: 0.4875\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3894 - val_loss: 0.4871\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4009 - val_loss: 0.4829\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4121 - val_loss: 0.4815\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3845 - val_loss: 0.4810\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3990 - val_loss: 0.4805\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4192 - val_loss: 0.4750\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3758 - val_loss: 0.4835\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3963 - val_loss: 0.4742\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3936 - val_loss: 0.4738\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3794 - val_loss: 0.4732\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3993 - val_loss: 0.4812\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3961 - val_loss: 0.4757\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3921 - val_loss: 0.4732\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3910 - val_loss: 0.4705\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3870 - val_loss: 0.4743\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3786 - val_loss: 0.4725\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3867 - val_loss: 0.4694\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3743 - val_loss: 0.4693\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3916 - val_loss: 0.4785\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3863 - val_loss: 0.4688\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3761 - val_loss: 0.4723\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3886 - val_loss: 0.4709\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3667 - val_loss: 0.4720\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3928 - val_loss: 0.4722\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3747 - val_loss: 0.4666\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3890 - val_loss: 0.4644\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3754 - val_loss: 0.4680\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3653 - val_loss: 0.4648\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3877 - val_loss: 0.4650\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3857 - val_loss: 0.4655\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3753 - val_loss: 0.4663\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3736 - val_loss: 0.4676\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3686 - val_loss: 0.4666\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3670 - val_loss: 0.4646\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3629 - val_loss: 0.4672\n",
      "121/121 [==============================] - 0s 981us/step - loss: 0.3746\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  55.4s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5591 - val_loss: 5.7622\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7257 - val_loss: 2.8152\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6690 - val_loss: 14.0642\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6818 - val_loss: 12.4891\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6079 - val_loss: 56.0532\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5531 - val_loss: 145.5585\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.2190 - val_loss: 329.9278\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.4221 - val_loss: 775.0319\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.1844 - val_loss: 1956.7959\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 37.0185 - val_loss: 4774.0391\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 72.7652 - val_loss: 10781.8994\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 24.0015 - val_loss: 24069.6719\n",
      "121/121 [==============================] - 0s 781us/step - loss: 62.7973\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   5.2s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.7511 - val_loss: 0.9704\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6190 - val_loss: 5.8133\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5615 - val_loss: 11.2757\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5505 - val_loss: 14.7146\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5371 - val_loss: 16.8750\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 18.2006\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5055 - val_loss: 18.7100\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5209 - val_loss: 19.4862\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5077 - val_loss: 19.5035\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4893 - val_loss: 18.0808\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5109 - val_loss: 19.0860\n",
      "121/121 [==============================] - 0s 758us/step - loss: 0.9577\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   4.7s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.5133 - val_loss: 12.0154\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7363 - val_loss: 28.5285\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7612 - val_loss: 13.8290\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0108 - val_loss: 11.4658\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6029 - val_loss: 0.5156\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5407 - val_loss: 2.2262\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5458 - val_loss: 0.8636\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5719 - val_loss: 0.9040\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5568 - val_loss: 2.3991\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 3.9928\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6221 - val_loss: 12.1324\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5569 - val_loss: 34.4494\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6281 - val_loss: 10.7828\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6370 - val_loss: 21.3716\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6388 - val_loss: 9.9135\n",
      "121/121 [==============================] - 0s 790us/step - loss: 0.4939\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   6.3s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8134 - val_loss: 29.7221\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6841 - val_loss: 21.1183\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 12.0139\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4966 - val_loss: 2.5590\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4166 - val_loss: 0.3702\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3824 - val_loss: 0.3663\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.3639\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.3620\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3626 - val_loss: 0.3872\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3668 - val_loss: 0.3699\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3532 - val_loss: 0.3580\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3409 - val_loss: 0.3527\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3704 - val_loss: 0.3699\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3565 - val_loss: 0.3679\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3598 - val_loss: 0.3639\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3542 - val_loss: 0.3582\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3635 - val_loss: 0.3613\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3681 - val_loss: 0.3586\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3518 - val_loss: 0.3612\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3547 - val_loss: 0.3603\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3467 - val_loss: 0.3554\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3401 - val_loss: 0.3631\n",
      "121/121 [==============================] - 0s 864us/step - loss: 0.3669\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=  10.4s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3921 - val_loss: 0.9200\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5140 - val_loss: 1.8001\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4386 - val_loss: 0.9561\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4244 - val_loss: 0.6007\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4144 - val_loss: 0.4109\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3989 - val_loss: 0.3981\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.5409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3889 - val_loss: 0.5922\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3791 - val_loss: 0.7621\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3847 - val_loss: 0.5113\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3796 - val_loss: 0.6919\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3609 - val_loss: 0.7086\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3607 - val_loss: 0.8526\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3519 - val_loss: 0.7429\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3819 - val_loss: 0.7189\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3658 - val_loss: 0.8119\n",
      "121/121 [==============================] - 0s 856us/step - loss: 0.3765\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=   7.5s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6030 - val_loss: 13.2616\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5112 - val_loss: 19.0616\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5216 - val_loss: 38.9989\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2864 - val_loss: 3.2661\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4195 - val_loss: 0.7735\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3883 - val_loss: 0.4989\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3793 - val_loss: 0.4570\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3992 - val_loss: 0.3846\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3763 - val_loss: 0.4409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3801 - val_loss: 0.3789\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3880 - val_loss: 0.4617\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3641 - val_loss: 0.4134\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3465 - val_loss: 0.3928\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.4289\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3799 - val_loss: 0.3720\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3630 - val_loss: 0.4217\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3515 - val_loss: 0.4111\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3607 - val_loss: 0.4211\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3769 - val_loss: 0.4182\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3640 - val_loss: 0.4656\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3609 - val_loss: 0.3805\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3735\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3516 - val_loss: 0.3966\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3467 - val_loss: 0.4350\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3409 - val_loss: 0.3490\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.3461\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3616 - val_loss: 0.4548\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3619\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4659 - val_loss: 0.3774\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3486 - val_loss: 0.3948\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3372 - val_loss: 0.3640\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3549\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3458 - val_loss: 0.3920\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3349 - val_loss: 0.3463\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3392 - val_loss: 0.3502\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3270 - val_loss: 0.3250\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3420 - val_loss: 0.3287\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3331 - val_loss: 0.3867\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3506 - val_loss: 0.3229\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3454 - val_loss: 0.3713\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3181 - val_loss: 0.3599\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3226 - val_loss: 0.4421\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.3510\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3474 - val_loss: 0.3347\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3307 - val_loss: 0.3472\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3242 - val_loss: 0.3504\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3329 - val_loss: 0.3312\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3207 - val_loss: 0.3157\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3436 - val_loss: 0.3565\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3273 - val_loss: 0.3521\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3340 - val_loss: 0.4718\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3211 - val_loss: 0.3230\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3271 - val_loss: 0.3275\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3224 - val_loss: 0.3174\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3467 - val_loss: 0.3313\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3330 - val_loss: 0.3648\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3286 - val_loss: 0.3370\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3338 - val_loss: 0.3927\n",
      "121/121 [==============================] - 0s 831us/step - loss: 0.3241\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=  26.9s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 4.7803 - val_loss: 5.2780\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.7128 - val_loss: 6.8657\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.7490 - val_loss: 7.2572\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3711 - val_loss: 6.3602\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1540 - val_loss: 4.9444\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0988 - val_loss: 4.0744\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0178 - val_loss: 3.1367\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9315 - val_loss: 2.4554\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8600 - val_loss: 1.9183\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8484 - val_loss: 1.5002\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7883 - val_loss: 1.1247\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7484 - val_loss: 0.8562\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7863 - val_loss: 0.7631\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7586 - val_loss: 0.7198\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7546 - val_loss: 0.6929\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7108 - val_loss: 0.6813\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7459 - val_loss: 0.6743\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7110 - val_loss: 0.6709\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6906 - val_loss: 0.6685\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6928 - val_loss: 0.6684\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6648 - val_loss: 0.6614\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6503 - val_loss: 0.6583\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6674 - val_loss: 0.6513\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6444 - val_loss: 0.6499\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6491 - val_loss: 0.6400\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6192 - val_loss: 0.6344\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6365 - val_loss: 0.6280\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6340 - val_loss: 0.6198\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6330 - val_loss: 0.6114\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6028 - val_loss: 0.6075\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5818 - val_loss: 0.6037\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5837 - val_loss: 0.5952\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5804 - val_loss: 0.5897\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5787 - val_loss: 0.5810\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5611 - val_loss: 0.5743\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5523 - val_loss: 0.5708\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5633 - val_loss: 0.5605\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5245 - val_loss: 0.5562\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5441 - val_loss: 0.5492\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5199 - val_loss: 0.5399\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5264 - val_loss: 0.5370\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5368 - val_loss: 0.5303\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5183 - val_loss: 0.5247\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5111 - val_loss: 0.5182\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5064 - val_loss: 0.5117\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5030 - val_loss: 0.5101\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5052 - val_loss: 0.5023\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4747 - val_loss: 0.4995\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4950 - val_loss: 0.4940\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4820 - val_loss: 0.4906\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4650 - val_loss: 0.4885\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4970 - val_loss: 0.4794\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4740 - val_loss: 0.4746\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4780 - val_loss: 0.4733\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4510 - val_loss: 0.4666\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4949 - val_loss: 0.4617\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4554 - val_loss: 0.4622\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4675 - val_loss: 0.4556\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4662 - val_loss: 0.4526\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4483 - val_loss: 0.4519\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4313 - val_loss: 0.4477\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4440 - val_loss: 0.4460\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4524 - val_loss: 0.4433\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4417 - val_loss: 0.4382\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4166 - val_loss: 0.4364\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4703 - val_loss: 0.4353\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4427 - val_loss: 0.4340\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4368 - val_loss: 0.4284\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4422 - val_loss: 0.4291\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4488 - val_loss: 0.4266\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4307 - val_loss: 0.4223\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4487 - val_loss: 0.4195\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4390 - val_loss: 0.4186\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4284 - val_loss: 0.4172\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4279 - val_loss: 0.4166\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4224 - val_loss: 0.4173\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4034 - val_loss: 0.4146\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4125 - val_loss: 0.4129\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4076 - val_loss: 0.4101\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4152 - val_loss: 0.4115\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3924 - val_loss: 0.4073\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4001 - val_loss: 0.4043\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3840 - val_loss: 0.4041\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4271 - val_loss: 0.4014\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4151 - val_loss: 0.4016\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3871 - val_loss: 0.3978\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4028 - val_loss: 0.3984\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4055 - val_loss: 0.3987\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3982 - val_loss: 0.3972\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3856 - val_loss: 0.3945\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4086 - val_loss: 0.3948\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4138 - val_loss: 0.3935\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4216 - val_loss: 0.3944\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4097 - val_loss: 0.3932\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3820 - val_loss: 0.3909\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3898 - val_loss: 0.3892\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3957 - val_loss: 0.3906\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3955 - val_loss: 0.3905\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3969 - val_loss: 0.3858\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.3864\n",
      "121/121 [==============================] - 0s 963us/step - loss: 0.4128\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  55.3s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 5.1679 - val_loss: 3.0809\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.6110 - val_loss: 2.3051\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.5596 - val_loss: 1.9873\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2203 - val_loss: 1.6202\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0152 - val_loss: 1.2920\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8867 - val_loss: 1.0720\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8261 - val_loss: 0.9242\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7442 - val_loss: 0.8181\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7253 - val_loss: 0.7495\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6788 - val_loss: 0.6979\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6603 - val_loss: 0.6645\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6346 - val_loss: 0.6342\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6126 - val_loss: 0.6101\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6008 - val_loss: 0.5944\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6393 - val_loss: 0.5775\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5985 - val_loss: 0.5656\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6002 - val_loss: 0.5542\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5921 - val_loss: 0.5445\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5778 - val_loss: 0.5354\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5715 - val_loss: 0.5277\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5608 - val_loss: 0.5203\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5555 - val_loss: 0.5146\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5552 - val_loss: 0.5089\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5358 - val_loss: 0.5037\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5157 - val_loss: 0.4989\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5202 - val_loss: 0.4943\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5363 - val_loss: 0.4904\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5324 - val_loss: 0.4865\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5207 - val_loss: 0.4822\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5218 - val_loss: 0.4779\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5076 - val_loss: 0.4747\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5038 - val_loss: 0.4708\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5038 - val_loss: 0.4673\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4883 - val_loss: 0.4629\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4799 - val_loss: 0.4590\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4774 - val_loss: 0.4557\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4875 - val_loss: 0.4520\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4605 - val_loss: 0.4483\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4829 - val_loss: 0.4447\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4483 - val_loss: 0.4412\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4446 - val_loss: 0.4379\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4541 - val_loss: 0.4347\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4759 - val_loss: 0.4314\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4664 - val_loss: 0.4287\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4442 - val_loss: 0.4264\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4456 - val_loss: 0.4238\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4484 - val_loss: 0.4219\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4268 - val_loss: 0.4208\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4510 - val_loss: 0.4208\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4346 - val_loss: 0.4194\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4492 - val_loss: 0.4173\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4511 - val_loss: 0.4183\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4359 - val_loss: 0.4188\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4414 - val_loss: 0.4194\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4214 - val_loss: 0.4226\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4502 - val_loss: 0.4249\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4475 - val_loss: 0.4249\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4493 - val_loss: 0.4324\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4492 - val_loss: 0.4355\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4295 - val_loss: 0.4389\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4335 - val_loss: 0.4461\n",
      "121/121 [==============================] - 0s 949us/step - loss: 0.4359\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  33.4s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 5.3120 - val_loss: 3.9055\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.7957 - val_loss: 4.4663\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6787 - val_loss: 4.3289\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4635 - val_loss: 3.7713\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1788 - val_loss: 2.8802\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0509 - val_loss: 2.2344\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9418 - val_loss: 1.7969\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9410 - val_loss: 1.5003\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8680 - val_loss: 1.3242\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7984 - val_loss: 1.1626\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7986 - val_loss: 1.0519\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7610 - val_loss: 0.9461\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7080 - val_loss: 0.8801\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7198 - val_loss: 0.8517\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7344 - val_loss: 0.8259\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7118 - val_loss: 0.8020\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6864 - val_loss: 0.7794\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7129 - val_loss: 0.7598\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6744 - val_loss: 0.7428\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6571 - val_loss: 0.7277\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6514 - val_loss: 0.7139\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6822 - val_loss: 0.7002\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6308 - val_loss: 0.6867\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6320 - val_loss: 0.6759\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6369 - val_loss: 0.6627\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6083 - val_loss: 0.6488\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6323 - val_loss: 0.6381\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6175 - val_loss: 0.6272\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5962 - val_loss: 0.6184\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6037 - val_loss: 0.6044\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5759 - val_loss: 0.5935\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6067 - val_loss: 0.5863\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5939 - val_loss: 0.5754\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5508 - val_loss: 0.5690\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5611 - val_loss: 0.5615\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5584 - val_loss: 0.5540\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5607 - val_loss: 0.5472\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5584 - val_loss: 0.5396\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5605 - val_loss: 0.5333\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5362 - val_loss: 0.5271\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5218 - val_loss: 0.5244\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5193 - val_loss: 0.5185\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5233 - val_loss: 0.5119\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5316 - val_loss: 0.5082\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5219 - val_loss: 0.5034\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5146 - val_loss: 0.4980\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5255 - val_loss: 0.4947\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5053 - val_loss: 0.4906\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5140 - val_loss: 0.4867\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4908 - val_loss: 0.4830\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5179 - val_loss: 0.4792\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4880 - val_loss: 0.4760\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4936 - val_loss: 0.4727\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4921 - val_loss: 0.4696\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4840 - val_loss: 0.4673\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4870 - val_loss: 0.4645\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4771 - val_loss: 0.4611\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4847 - val_loss: 0.4582\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4828 - val_loss: 0.4552\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4940 - val_loss: 0.4524\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4945 - val_loss: 0.4507\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4668 - val_loss: 0.4480\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4598 - val_loss: 0.4451\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4850 - val_loss: 0.4427\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4839 - val_loss: 0.4408\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4485 - val_loss: 0.4390\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4581 - val_loss: 0.4362\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4620 - val_loss: 0.4341\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4347 - val_loss: 0.4323\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4566 - val_loss: 0.4299\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4720 - val_loss: 0.4280\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4307 - val_loss: 0.4263\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4450 - val_loss: 0.4242\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4413 - val_loss: 0.4236\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4319 - val_loss: 0.4218\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4504 - val_loss: 0.4199\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4393 - val_loss: 0.4179\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4322 - val_loss: 0.4163\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4346 - val_loss: 0.4153\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4242 - val_loss: 0.4154\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4227 - val_loss: 0.4139\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4287 - val_loss: 0.4117\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4160 - val_loss: 0.4108\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4352 - val_loss: 0.4100\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4314 - val_loss: 0.4084\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4154 - val_loss: 0.4080\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4217 - val_loss: 0.4072\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4119 - val_loss: 0.4062\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4275 - val_loss: 0.4059\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4162 - val_loss: 0.4044\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4234 - val_loss: 0.4029\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4082 - val_loss: 0.4040\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3991 - val_loss: 0.4032\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4225 - val_loss: 0.4024\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4192 - val_loss: 0.4024\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4019 - val_loss: 0.4018\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4073 - val_loss: 0.4018\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3959 - val_loss: 0.4019\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3938 - val_loss: 0.4013\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3989 - val_loss: 0.4015\n",
      "121/121 [==============================] - 0s 997us/step - loss: 0.4041\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  55.5s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.9516 - val_loss: 0.7997\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7190 - val_loss: 15.7812\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7257 - val_loss: 35.0956\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7447 - val_loss: 2.6499\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6193 - val_loss: 0.5883\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5216 - val_loss: 0.5227\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5009 - val_loss: 0.4580\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4765 - val_loss: 0.4403\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4480 - val_loss: 0.4225\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4371 - val_loss: 0.4113\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4201 - val_loss: 0.4005\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3976 - val_loss: 0.3963\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4252 - val_loss: 0.3889\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4105 - val_loss: 0.3956\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4078 - val_loss: 0.3920\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3997 - val_loss: 0.3925\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4173 - val_loss: 0.3976\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4164 - val_loss: 0.3862\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3906 - val_loss: 0.3895\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3947 - val_loss: 0.3869\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3912 - val_loss: 0.3961\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3808 - val_loss: 0.3959\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3955 - val_loss: 0.4031\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3816 - val_loss: 0.3818\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3869 - val_loss: 0.3987\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3724 - val_loss: 0.3908\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3899 - val_loss: 0.3831\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.3910\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3844 - val_loss: 0.4019\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3750 - val_loss: 0.4031\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.3877\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3682 - val_loss: 0.3832\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3636 - val_loss: 0.3761\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3643 - val_loss: 0.3808\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3613 - val_loss: 0.3718\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3609 - val_loss: 0.3769\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3640 - val_loss: 0.3660\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3641 - val_loss: 0.3794\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3594 - val_loss: 0.3646\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3463 - val_loss: 0.3743\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3577 - val_loss: 0.3687\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3649\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3607 - val_loss: 0.3623\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3592 - val_loss: 0.3659\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3628 - val_loss: 0.3716\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3429 - val_loss: 0.3677\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3567 - val_loss: 0.3653\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3397 - val_loss: 0.3630\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3674 - val_loss: 0.3628\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3543 - val_loss: 0.3670\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3396 - val_loss: 0.3669\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3666 - val_loss: 0.3597\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3502 - val_loss: 0.3617\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3578 - val_loss: 0.3658\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3442 - val_loss: 0.3568\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3758 - val_loss: 0.3585\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3572 - val_loss: 0.3621\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3531 - val_loss: 0.3562\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3546\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3569\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3404 - val_loss: 0.3550\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3442 - val_loss: 0.3727\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3596 - val_loss: 0.3519\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3431 - val_loss: 0.3603\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3313 - val_loss: 0.3668\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3750 - val_loss: 0.3635\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3564\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3479 - val_loss: 0.3541\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3564 - val_loss: 0.3492\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.3511\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3519 - val_loss: 0.3526\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3728 - val_loss: 0.3452\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3588 - val_loss: 0.3470\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3503\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3486 - val_loss: 0.3507\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3455 - val_loss: 0.3544\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3404 - val_loss: 0.3485\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3437 - val_loss: 0.3544\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3460 - val_loss: 0.3453\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3466 - val_loss: 0.3480\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3317 - val_loss: 0.3499\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3359 - val_loss: 0.3480\n",
      "121/121 [==============================] - 0s 856us/step - loss: 0.3737\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  38.2s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9660 - val_loss: 1.8423\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5612 - val_loss: 0.7206\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4694 - val_loss: 0.5510\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4515 - val_loss: 0.4658\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4428 - val_loss: 0.4844\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4243 - val_loss: 0.6057\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4046 - val_loss: 0.6684\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4149 - val_loss: 0.7664\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4012 - val_loss: 0.9608\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4072 - val_loss: 0.9923\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4061 - val_loss: 1.1373\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3861 - val_loss: 1.1177\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3899 - val_loss: 1.2480\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3782 - val_loss: 1.4285\n",
      "121/121 [==============================] - 0s 866us/step - loss: 0.4222\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=   6.8s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.7409 - val_loss: 41.8632\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8393 - val_loss: 0.8782\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.598 - 0s 2ms/step - loss: 0.5977 - val_loss: 0.6405\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5130 - val_loss: 0.5348\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4688 - val_loss: 0.4786\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4307 - val_loss: 0.4417\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4237 - val_loss: 0.4258\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4371 - val_loss: 0.4107\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4154 - val_loss: 0.4032\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4112 - val_loss: 0.3984\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4297 - val_loss: 0.3938\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3983 - val_loss: 0.3910\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3731 - val_loss: 0.3926\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3863 - val_loss: 0.3903\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4142 - val_loss: 0.3882\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3945 - val_loss: 0.3881\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3808 - val_loss: 0.3921\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3960 - val_loss: 0.3921\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4046 - val_loss: 0.3901\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 0.3899\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3845 - val_loss: 0.3883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4007 - val_loss: 0.3848\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3812 - val_loss: 0.3858\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3786 - val_loss: 0.3882\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3836\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.3871\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3907 - val_loss: 0.3852\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3853 - val_loss: 0.3870\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3772 - val_loss: 0.3914\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3803 - val_loss: 0.3867\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3647 - val_loss: 0.3895\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3950 - val_loss: 0.3900\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3712 - val_loss: 0.4059\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3583 - val_loss: 0.3921\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3914\n",
      "121/121 [==============================] - 0s 860us/step - loss: 0.3666\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  16.1s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 6.9930 - val_loss: 15.3277\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.4388 - val_loss: 10.6458\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3229 - val_loss: 7.4741\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.4675 - val_loss: 5.3221\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7077 - val_loss: 3.8599\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.2495 - val_loss: 2.8520\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9692 - val_loss: 2.1656\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.6716 - val_loss: 1.6996\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4539 - val_loss: 1.3890\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2488 - val_loss: 1.1739\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0994 - val_loss: 1.0312\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9905 - val_loss: 0.9390\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9953 - val_loss: 0.8805\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8896 - val_loss: 0.8473\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8712 - val_loss: 0.8263\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7897 - val_loss: 0.8192\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7982 - val_loss: 0.8197\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7672 - val_loss: 0.8269\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7350 - val_loss: 0.8376\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7344 - val_loss: 0.8491\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6966 - val_loss: 0.8520\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6855 - val_loss: 0.8647\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6959 - val_loss: 0.8765\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6770 - val_loss: 0.8871\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6826 - val_loss: 0.8923\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.8903\n",
      "121/121 [==============================] - 0s 773us/step - loss: 0.6591\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  10.7s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 7.4296 - val_loss: 30.2588\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.5856 - val_loss: 25.8363\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.2452 - val_loss: 22.1571\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.2208 - val_loss: 19.0498\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.5416 - val_loss: 16.4025\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.1175 - val_loss: 14.1257\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.7739 - val_loss: 12.1500\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4983 - val_loss: 10.4264\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3352 - val_loss: 8.9212\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1511 - val_loss: 7.6048\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0374 - val_loss: 6.4492\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9682 - val_loss: 5.4389\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8876 - val_loss: 4.5570\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8435 - val_loss: 3.7953\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8670 - val_loss: 3.1367\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7779 - val_loss: 2.5722\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7894 - val_loss: 2.0952\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7539 - val_loss: 1.6967\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7201 - val_loss: 1.3693\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7203 - val_loss: 1.1092\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6997 - val_loss: 0.9086\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7026 - val_loss: 0.7644\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7028 - val_loss: 0.6709\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.6243\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6630 - val_loss: 0.6202\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.6553\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6652 - val_loss: 0.7264\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6662 - val_loss: 0.8297\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 0.9634\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6453 - val_loss: 1.1245\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6346 - val_loss: 1.3098\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6419 - val_loss: 1.5169\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6359 - val_loss: 1.7443\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6070 - val_loss: 1.9886\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6097 - val_loss: 2.2502\n",
      "121/121 [==============================] - 0s 765us/step - loss: 0.6426\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  14.2s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 6.8585 - val_loss: 38.3852\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0908 - val_loss: 26.3105\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.7199 - val_loss: 18.2127\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.1627 - val_loss: 12.8955\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.2087 - val_loss: 9.1600\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.7727 - val_loss: 6.6784\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3956 - val_loss: 4.9920\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3442 - val_loss: 3.8624\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0602 - val_loss: 3.0348\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8989 - val_loss: 2.4197\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8554 - val_loss: 2.0328\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7617 - val_loss: 1.7225\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 1.4887\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 1.3335\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 1.2074\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6263 - val_loss: 1.1344\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5998 - val_loss: 1.0635\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6085 - val_loss: 1.0073\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.9847\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.9534\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5634 - val_loss: 0.9285\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5958 - val_loss: 0.9114\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5692 - val_loss: 0.8864\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5526 - val_loss: 0.8722\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5542 - val_loss: 0.8594\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5585 - val_loss: 0.8418\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.8522\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5653 - val_loss: 0.8465\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5563 - val_loss: 0.8516\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5632 - val_loss: 0.8468\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 0.8475\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5794 - val_loss: 0.8518\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5685 - val_loss: 0.8545\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5430 - val_loss: 0.8589\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5453 - val_loss: 0.8619\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5529 - val_loss: 0.8454\n",
      "121/121 [==============================] - 0s 766us/step - loss: 0.5604\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  14.8s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8014 - val_loss: 2.0932\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7118 - val_loss: 3.4021\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6227 - val_loss: 1.7173\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5516 - val_loss: 1.3191\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4920 - val_loss: 0.7136\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4592 - val_loss: 0.8040\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4444 - val_loss: 0.4312\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4255 - val_loss: 0.6083\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4086 - val_loss: 0.3846\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3977 - val_loss: 0.5372\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3811 - val_loss: 0.3696\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3661 - val_loss: 0.3890\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3918 - val_loss: 0.5037\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3762 - val_loss: 0.4142\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3690 - val_loss: 0.4111\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3652 - val_loss: 0.4088\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3710 - val_loss: 0.4695\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3762 - val_loss: 0.3829\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3500 - val_loss: 0.4445\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 0.3913\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3530 - val_loss: 0.3828\n",
      "121/121 [==============================] - 0s 931us/step - loss: 0.3737\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=  11.1s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6368 - val_loss: 3.9918\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 1.2041\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.6442\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4911 - val_loss: 0.4769\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4659 - val_loss: 0.4388\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4336 - val_loss: 0.5192\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4075 - val_loss: 0.7016\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4075 - val_loss: 0.7233\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3924 - val_loss: 0.9900\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3926 - val_loss: 0.8728\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3873 - val_loss: 1.0053\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3679 - val_loss: 0.9835\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3676 - val_loss: 0.9997\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3550 - val_loss: 1.1486\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3828 - val_loss: 0.9536\n",
      "121/121 [==============================] - 0s 891us/step - loss: 0.3847\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=   7.9s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.2134 - val_loss: 12.6894\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6351 - val_loss: 6.7383\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5484 - val_loss: 0.5545\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4739 - val_loss: 0.4484\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4495 - val_loss: 0.4266\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4155 - val_loss: 0.4275\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4133 - val_loss: 0.4065\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4281 - val_loss: 0.3913\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3995 - val_loss: 0.4179\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3925 - val_loss: 0.3668\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3984 - val_loss: 0.4375\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3767 - val_loss: 0.3775\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3525 - val_loss: 0.3611\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3616 - val_loss: 0.4205\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3881 - val_loss: 0.3548\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3681 - val_loss: 0.4051\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3550 - val_loss: 0.3579\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3666 - val_loss: 0.3584\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3771 - val_loss: 0.3879\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3662 - val_loss: 0.3690\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3571 - val_loss: 0.3605\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3540\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3530 - val_loss: 0.3443\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3478 - val_loss: 0.3856\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3470 - val_loss: 0.3308\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3425 - val_loss: 0.3345\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3626 - val_loss: 0.3944\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3542 - val_loss: 0.3349\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3419 - val_loss: 0.3918\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3458 - val_loss: 0.3433\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3324 - val_loss: 0.3327\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3568 - val_loss: 0.3988\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3427 - val_loss: 0.3482\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3272 - val_loss: 0.3683\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3329 - val_loss: 0.3275\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3257 - val_loss: 0.3484\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3362 - val_loss: 0.3284\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3300 - val_loss: 0.4097\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3433 - val_loss: 0.3241\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3368 - val_loss: 0.3500\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3098 - val_loss: 0.3629\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3169 - val_loss: 0.3461\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3361 - val_loss: 0.3257\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3389 - val_loss: 0.3568\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3208 - val_loss: 0.3241\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3190 - val_loss: 0.3384\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 0.3283\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3198 - val_loss: 0.3186\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3280 - val_loss: 0.3859\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3208 - val_loss: 0.3186\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3302 - val_loss: 0.3492\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3168 - val_loss: 0.3136\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3231 - val_loss: 0.3115\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3179 - val_loss: 0.3104\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3153 - val_loss: 0.3200\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3234 - val_loss: 0.3122\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3167 - val_loss: 0.3589\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3215 - val_loss: 0.3077\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3217 - val_loss: 0.3710\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3236 - val_loss: 0.3157\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3263 - val_loss: 0.3491\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3135 - val_loss: 0.3128\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3099 - val_loss: 0.3044\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3294 - val_loss: 0.3095\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3142 - val_loss: 0.3036\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3019 - val_loss: 0.3769\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3166 - val_loss: 0.3239\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3249 - val_loss: 0.4679\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3027 - val_loss: 0.4778\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3107 - val_loss: 0.4382\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3235 - val_loss: 0.3089\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2934 - val_loss: 0.3363\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3106 - val_loss: 0.3180\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3069 - val_loss: 0.3378\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2992 - val_loss: 0.3057\n",
      "121/121 [==============================] - 0s 889us/step - loss: 0.3172\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=  37.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 10.9min finished\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001EE9B169910>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-67ebe7c7ddb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                    \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                    verbose=2)\n\u001b[1;32m---> 14\u001b[1;33m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[0m\u001b[0;32m     15\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;31m# we clone again after setting params in case some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[0;32m    762\u001b[0m                 **self.best_params_))\n\u001b[0;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[0;32m     97\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                                (estimator, name))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001EE9B169910>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, \n",
    "                                   n_iter=10, \n",
    "                                   cv=3, \n",
    "                                   verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T00:58:50.430805Z",
     "start_time": "2021-03-15T00:58:50.272226Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnd_search_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f97e75618950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnd_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rnd_search_cv' is not defined"
     ]
    }
   ],
   "source": [
    "rnd_search_cv.best_params_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习：基于MLP的MNIST分类任务\n",
    "\n",
    "> todo:\n",
    "> - load dataset\n",
    "> - normalize dataset\n",
    "> - create exponential learning rate callback\n",
    "> - create model using sequential\n",
    "> - train model\n",
    "> - create plot loss function vs learning rate\n",
    "> - based loss and learning rate relation to adjust learning rate \n",
    "> - create early stopping, model checkpoint, tensorboard callback\n",
    "> - re-training model\n",
    "> - from checkpoint load best model\n",
    "> - evalute model\n",
    "> - using tensorboard check learning curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T07:44:19.588157Z",
     "start_time": "2021-03-15T07:44:17.752556Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T07:44:20.415680Z",
     "start_time": "2021-03-15T07:44:20.404710Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_run_logdir(logpath):\n",
    "    root_logdir = os.path.join(os.curdir, logpath)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(root_logdir)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % root_logdir)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % root_logdir)\n",
    "\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T07:44:22.671353Z",
     "start_time": "2021-03-15T07:44:21.606159Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mnist = keras.datasets.mnist.load_data()\n",
    "\n",
    "# normalize dataset\n",
    "(X_train_all, y_train_all), (X_test, y_test) = mnist\n",
    "X_train, X_valid, X_test = X_train_all[:50000]/255.0, X_train_all[50000:]/255.0, X_test/255.0\n",
    "y_train, y_valid = y_train_all[:50000], y_train_all[50000:]\n",
    "\n",
    "K = keras.backend\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "        self.factor = factor\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.losses.append(logs['loss'])\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "\n",
    "# create model using sequential\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "#               optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# expon_lr = ExponentialLearningRate(factor=1.005)\n",
    "\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     epochs=1,\n",
    "#                     validation_data=(X_valid, y_valid),\n",
    "#                     callbacks=[expon_lr])\n",
    "\n",
    "# plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "# plt.gca().set_xscale('log')\n",
    "# plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\n",
    "# plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\n",
    "# plt.xlabel(\"Learning rate\")\n",
    "# plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T07:45:47.522388Z",
     "start_time": "2021-03-15T07:44:37.104720Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory .\\mnist_logs failed\n",
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6749 - accuracy: 0.7965 - val_loss: 0.1849 - val_accuracy: 0.9491\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1869 - accuracy: 0.9443 - val_loss: 0.1536 - val_accuracy: 0.9570\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1460 - accuracy: 0.9568 - val_loss: 0.1353 - val_accuracy: 0.9621\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1189 - accuracy: 0.9648 - val_loss: 0.1316 - val_accuracy: 0.9636\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1049 - accuracy: 0.9696 - val_loss: 0.1221 - val_accuracy: 0.9652\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0940 - accuracy: 0.9722 - val_loss: 0.1242 - val_accuracy: 0.9638\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0798 - accuracy: 0.9758 - val_loss: 0.1325 - val_accuracy: 0.9631\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0748 - accuracy: 0.9779 - val_loss: 0.1213 - val_accuracy: 0.9661\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0654 - accuracy: 0.9794 - val_loss: 0.1177 - val_accuracy: 0.9675\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0610 - accuracy: 0.9812 - val_loss: 0.1277 - val_accuracy: 0.9666\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0607 - accuracy: 0.9813 - val_loss: 0.1207 - val_accuracy: 0.9683\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0529 - accuracy: 0.9840 - val_loss: 0.1249 - val_accuracy: 0.9680\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0498 - accuracy: 0.9841 - val_loss: 0.1244 - val_accuracy: 0.9693\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0484 - accuracy: 0.9856 - val_loss: 0.1283 - val_accuracy: 0.9681\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0449 - accuracy: 0.9870 - val_loss: 0.1393 - val_accuracy: 0.9643\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0403 - accuracy: 0.9871 - val_loss: 0.1439 - val_accuracy: 0.9657\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0404 - accuracy: 0.9879 - val_loss: 0.1301 - val_accuracy: 0.9699\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0344 - accuracy: 0.9883 - val_loss: 0.1322 - val_accuracy: 0.9679\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0315 - accuracy: 0.9897 - val_loss: 0.1307 - val_accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "# keras.backend.clear_session()\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelcheck_cb = keras.callbacks.ModelCheckpoint('models/mnist_keras_model.h5',\n",
    "                                                save_best_only=True)\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                             restore_best_weights=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir('mnist_logs'))\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[modelcheck_cb, earlystop_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T07:46:23.376281Z",
     "start_time": "2021-03-15T07:46:22.858666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11641941219568253, 0.9668999910354614]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.088px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
